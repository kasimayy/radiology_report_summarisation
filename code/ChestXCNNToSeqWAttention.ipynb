{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1210 14:06:53.895703 139990078469888 __init__.py:321] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "W1210 14:06:54.049607 139990078469888 __init__.py:321] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "W1210 14:06:54.137888 139990078469888 __init__.py:352] Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1210 14:06:54.458840 139990078469888 __init__.py:321] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import json\n",
    "import sys\n",
    "# sys.path.append(\"..\")\n",
    "from utils import data_proc_tools as dpt\n",
    "from utils import plot_tools as pt\n",
    "from utils.custom_metrics import recall, precision, binary_accuracy\n",
    "from utils.custom_metrics import recall_np, precision_np, binary_accuracy_np, multilabel_confusion_matrix\n",
    "import random\n",
    "random.seed(42)\n",
    "random_state=1000\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import pylab\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (8.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "dir = '/vol/medic02/users/ag6516/radiology_report_summarisation/'\n",
    "data_dir = dir + 'data/'\n",
    "\n",
    "aug = 'aug'\n",
    "\n",
    "model_output_dir = dir + 'trained_models/cnn2hierseq_att/'\n",
    "\n",
    "train_df = pd.read_pickle(data_dir + 'train/{}_train.pkl'.format(aug))\n",
    "val_df = pd.read_pickle(data_dir + 'val/val.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare sequence data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>examid</th>\n",
       "      <th>report</th>\n",
       "      <th>all_mesh</th>\n",
       "      <th>single_mesh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CXR1000_IM-0003</td>\n",
       "      <td>[increased, opacity, within, right, upper, lobe, possible, mass, associated, area, atelectasis, focal, consolidation, ., cardiac, silhouette, within, normal, limits, ., opacity, left, midlung, overlying, posterior, left, 5th, rib, may, represent, focal, airspace, disease, ., increased, opacity, right, upper, lobe, associated, atelectasis, may, represent, focal, consolidation, mass, lesion, atelectasis, ., recommend, chest, ct, evaluation, ., opacity, overlying, left, 5th, rib, may, represent, focal, airspace, disease]</td>\n",
       "      <td>[opacity, lung, lingula, opacity, lung, upper_lobe, right, pulmonary_atelectasis, upper_lobe, right]</td>\n",
       "      <td>[opacity, lung, upper_lobe, right]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CXR1001_IM-0004</td>\n",
       "      <td>[interstitial, markings, diffusely, prominent, throughout, lungs, ., heart, size, normal, ., pulmonary, normal, ., diffuse, fibrosis]</td>\n",
       "      <td>[diffuse, markings, lung, bilateral, interstitial, diffuse, prominent]</td>\n",
       "      <td>[markings, lung, bilateral, interstitial, diffuse, prominent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CXR1002_IM-0004</td>\n",
       "      <td>[status, post, left, mastectomy, ., heart, size, normal, ., lungs, clear]</td>\n",
       "      <td>[left]</td>\n",
       "      <td>[left]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CXR1003_IM-0005</td>\n",
       "      <td>[heart, size, pulmonary, vascularity, appear, within, normal, limits, ., retrocardiac, soft, tissue, density, present, ., appears, air, within, suggest, represents, hiatal, hernia, ., vascular, calcification, noted, ., calcified, granuloma, seen, ., interval, development, bandlike, opacity, left, lung, base, ., may, represent, atelectasis, ., osteopenia, present, spine, ., retrocardiac, soft, tissue, density, ., appearance, suggests, hiatal, hernia, ., left, base, bandlike, opacity, ., appearance, suggests, atelectasis]</td>\n",
       "      <td>[bone_diseases_metabolic, spine, calcified_granuloma, calcinosis, blood_vessels, density, retrocardiac, opacity, lung, base, left]</td>\n",
       "      <td>[opacity, lung, base, left]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CXR1004_IM-0005</td>\n",
       "      <td>[heart, ,, pulmonary, mediastinum, within, normal, limits, ., aorta, tortuous, ectatic, ., degenerative, changes, acromioclavicular, joints, ., degenerative, changes, spine, ., ivc, identified]</td>\n",
       "      <td>[aorta, tortuous, catheters_indwelling, shoulder, bilateral, degenerative, spine, degenerative]</td>\n",
       "      <td>[shoulder, bilateral, degenerative]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            examid  \\\n",
       "0  CXR1000_IM-0003   \n",
       "1  CXR1001_IM-0004   \n",
       "2  CXR1002_IM-0004   \n",
       "3  CXR1003_IM-0005   \n",
       "4  CXR1004_IM-0005   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          report  \\\n",
       "0  [increased, opacity, within, right, upper, lobe, possible, mass, associated, area, atelectasis, focal, consolidation, ., cardiac, silhouette, within, normal, limits, ., opacity, left, midlung, overlying, posterior, left, 5th, rib, may, represent, focal, airspace, disease, ., increased, opacity, right, upper, lobe, associated, atelectasis, may, represent, focal, consolidation, mass, lesion, atelectasis, ., recommend, chest, ct, evaluation, ., opacity, overlying, left, 5th, rib, may, represent, focal, airspace, disease]     \n",
       "1  [interstitial, markings, diffusely, prominent, throughout, lungs, ., heart, size, normal, ., pulmonary, normal, ., diffuse, fibrosis]                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "2  [status, post, left, mastectomy, ., heart, size, normal, ., lungs, clear]                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "3  [heart, size, pulmonary, vascularity, appear, within, normal, limits, ., retrocardiac, soft, tissue, density, present, ., appears, air, within, suggest, represents, hiatal, hernia, ., vascular, calcification, noted, ., calcified, granuloma, seen, ., interval, development, bandlike, opacity, left, lung, base, ., may, represent, atelectasis, ., osteopenia, present, spine, ., retrocardiac, soft, tissue, density, ., appearance, suggests, hiatal, hernia, ., left, base, bandlike, opacity, ., appearance, suggests, atelectasis]   \n",
       "4  [heart, ,, pulmonary, mediastinum, within, normal, limits, ., aorta, tortuous, ectatic, ., degenerative, changes, acromioclavicular, joints, ., degenerative, changes, spine, ., ivc, identified]                                                                                                                                                                                                                                                                                                                                               \n",
       "\n",
       "                                                                                                                             all_mesh  \\\n",
       "0  [opacity, lung, lingula, opacity, lung, upper_lobe, right, pulmonary_atelectasis, upper_lobe, right]                                 \n",
       "1  [diffuse, markings, lung, bilateral, interstitial, diffuse, prominent]                                                               \n",
       "2  [left]                                                                                                                               \n",
       "3  [bone_diseases_metabolic, spine, calcified_granuloma, calcinosis, blood_vessels, density, retrocardiac, opacity, lung, base, left]   \n",
       "4  [aorta, tortuous, catheters_indwelling, shoulder, bilateral, degenerative, spine, degenerative]                                      \n",
       "\n",
       "                                                     single_mesh  \n",
       "0  [opacity, lung, upper_lobe, right]                             \n",
       "1  [markings, lung, bilateral, interstitial, diffuse, prominent]  \n",
       "2  [left]                                                         \n",
       "3  [opacity, lung, base, left]                                    \n",
       "4  [shoulder, bilateral, degenerative]                            "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepend and append start and end tokens to mesh captions and text reports\n",
    "start_token = 'start'\n",
    "end_token = 'end'\n",
    "unknown_token = '**unknown**'\n",
    "max_mesh_length = 13 # avg. + 1std. + start + end\n",
    "max_report_length = 37 # avg. + 1std. + start + end\n",
    "\n",
    "train_df['pad_mesh_caption'] = train_df.all_mesh.apply(lambda x: dpt.pad_sequence(x, max_mesh_length, start_token, end_token))\n",
    "train_df['pad_text_report'] = train_df.report.apply(lambda x: dpt.pad_sequence(x, max_report_length, start_token, end_token))\n",
    "\n",
    "val_df['pad_mesh_caption'] = val_df.all_mesh.apply(lambda x: dpt.pad_sequence(x, max_mesh_length, start_token, end_token))\n",
    "val_df['pad_text_report'] = val_df.report.apply(lambda x: dpt.pad_sequence(x, max_report_length, start_token, end_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorise text reports and mesh captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mesh = list(train_df.pad_mesh_caption)\n",
    "train_reports = list(train_df.pad_text_report)\n",
    "\n",
    "# vectorize mesh captions\n",
    "dpt.mesh_to_vectors(train_mesh, dicts_dir=data_dir+'dicts/', \n",
    "                    load_dicts=True, save=True, output_dir=data_dir+'train/')\n",
    "\n",
    "# vectorise reports\n",
    "dpt.reports_to_vectors(train_reports, dicts_dir=data_dir+'dicts/', \n",
    "                       load_dicts=True, save=True, output_dir=data_dir+'train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id, id_to_word = dpt.load_report_dicts(data_dir+'dicts/')\n",
    "mesh_to_id, id_to_mesh = dpt.load_mesh_dicts(data_dir+'dicts/')\n",
    "\n",
    "report_vocab_length = len(word_to_id)\n",
    "mesh_vocab_length = len(mesh_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1475, 128)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_vocab_length, mesh_vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays of indixes for input sentences, output entities and shifted output entities (t-1)\n",
    "train_token_ids_array = np.load(data_dir + 'train/token_ids_array.npy')\n",
    "train_mesh_ids_array = np.load(data_dir + 'train/mesh_ids_array.npy')\n",
    "train_mesh_ids_array_shifted =[np.concatenate((mesh_to_id[start_token], t[:-1]), axis=None) for t in train_mesh_ids_array]\n",
    "train_mesh_ids_array_shifted = np.asarray(train_mesh_ids_array_shifted)\n",
    "\n",
    "val_token_ids_array = np.load(data_dir + 'val/token_ids_array.npy')\n",
    "val_mesh_ids_array = np.load(data_dir + 'val/mesh_ids_array.npy')\n",
    "val_mesh_ids_array_shifted = [np.concatenate((mesh_to_id[start_token], t[:-1]), axis=None) for t in val_mesh_ids_array]\n",
    "val_mesh_ids_array_shifted = np.asarray(val_mesh_ids_array_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot-encode\n",
    "one_hot_reports_train = dpt.one_hot_sequence(train_token_ids_array, report_vocab_length)\n",
    "one_hot_mesh_train = dpt.one_hot_sequence(train_mesh_ids_array, mesh_vocab_length)\n",
    "one_hot_mesh_shifted_train = dpt.one_hot_sequence(train_mesh_ids_array_shifted, mesh_vocab_length)\n",
    "\n",
    "one_hot_reports_val = dpt.one_hot_sequence(val_token_ids_array, report_vocab_length)\n",
    "one_hot_mesh_val = dpt.one_hot_sequence(val_mesh_ids_array, mesh_vocab_length)\n",
    "one_hot_mesh_shifted_val = dpt.one_hot_sequence(val_mesh_ids_array_shifted, mesh_vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5148, 37), (5148, 13, 128), (5148, 13, 128))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_token_ids_array.shape, one_hot_mesh_train.shape, one_hot_mesh_shifted_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CNN-to-Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word encoder emb:  (None, 37, 1475)\n",
      "Word conv1 output:  (None, 37, 64)\n",
      "Word conv2 output:  (None, 37, 64)\n",
      "Word conv3 output:  (None, 37, 64)\n",
      "Word Concat:  (None, 37, 192)\n",
      "Max pool outputs:  (None, 37, 192)\n",
      "Phrase Concat:  (None, 37, 640)\n",
      "Encoder states:  (None, 640)\n",
      "Decoder outputs:  (None, 13, 640)\n",
      "Attention outputs:  (None, 13, 640)\n",
      "Model: \"model_17\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_29 (InputLayer)           [(None, 37, 1475)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 37, 64)       94464       input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 37, 64)       283264      input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 37, 64)       472064      input_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "word_concat_layer (Concatenate) (None, 37, 192)      0           conv1d_24[0][0]                  \n",
      "                                                                 conv1d_25[0][0]                  \n",
      "                                                                 conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 37, 128)      24704       word_concat_layer[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 37, 128)      73856       word_concat_layer[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 37, 128)      123008      word_concat_layer[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 37, 192)      0           word_concat_layer[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "phrase_concat_layer (Concatenat (None, 37, 640)      0           conv1d_24[0][0]                  \n",
      "                                                                 conv1d_27[0][0]                  \n",
      "                                                                 conv1d_28[0][0]                  \n",
      "                                                                 conv1d_29[0][0]                  \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_30 (InputLayer)           [(None, 13, 128)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 640)          0           phrase_concat_layer[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   multiple             1968640     input_30[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer multiple             819840      phrase_concat_layer[0][0]        \n",
      "                                                                 lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dec_concat_layer (Concatenate)  (None, 13, 1280)     0           lstm_4[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_8 (TimeDistrib (None, 13, 128)      163968      dec_concat_layer[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 4,023,808\n",
      "Trainable params: 4,023,808\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from utils.custom_metrics import recall, precision, binary_accuracy\n",
    "from utils.cnn_textsum_models import HierCNN2SeqAtt\n",
    "\n",
    "input_dim = len(word_to_id)\n",
    "output_dim = len(mesh_to_id)\n",
    "embedding_dim = 256\n",
    "word_conv_dim = 64\n",
    "phrase_conv_dim = 128\n",
    "conv1_kernel = 1\n",
    "conv2_kernel = 3\n",
    "conv3_kernel = 5\n",
    "input_seq_length = max_report_length\n",
    "output_seq_length = max_mesh_length\n",
    "epochs = 70\n",
    "optimizer = 'adam'\n",
    "loss='categorical_crossentropy'\n",
    "batch_size = 128\n",
    "\n",
    "new_experiment = HierCNN2SeqAtt(epochs=epochs,\n",
    "                               metrics=['accuracy', binary_accuracy,recall,precision],\n",
    "                               optimizer=optimizer,\n",
    "                               loss=loss,\n",
    "                               batch_size=batch_size, \n",
    "                               input_dim=input_dim,\n",
    "                               output_dim=output_dim,\n",
    "                               embedding_dim=embedding_dim,\n",
    "                               word_conv_dim=word_conv_dim,\n",
    "                               phrase_conv_dim=phrase_conv_dim,\n",
    "                               conv1_kernel=conv1_kernel,\n",
    "                               conv2_kernel=conv2_kernel,\n",
    "                               conv3_kernel=conv3_kernel,\n",
    "                               input_seq_length=input_seq_length,\n",
    "                               output_seq_length=output_seq_length,\n",
    "                               verbose=True)\n",
    "\n",
    "new_experiment.build_model()\n",
    "new_experiment.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word encoder emb:  (None, 37, 1475)\n",
      "Word conv1 output:  (None, 37, 64)\n",
      "Word conv2 output:  (None, 37, 64)\n",
      "Word conv3 output:  (None, 37, 64)\n",
      "Word Concat:  (None, 37, 192)\n",
      "Max pool outputs:  (None, 37, 192)\n",
      "Phrase Concat:  (None, 37, 640)\n",
      "Encoder states:  (None, 640)\n",
      "Decoder outputs:  (None, 13, 640)\n",
      "Attention outputs:  (None, 13, 640)\n",
      "Train on 5148 samples, validate on 300 samples\n",
      "Epoch 1/70\n",
      "5148/5148 [==============================] - 72s 14ms/sample - loss: 2.5433 - accuracy: 0.5538 - binary_accuracy: 0.9947 - recall: 0.4147 - precision: 0.7289 - val_loss: 1.5692 - val_accuracy: 0.6921 - val_binary_accuracy: 0.9965 - val_recall: 0.6066 - val_precision: 0.9202\n",
      "Epoch 2/70\n",
      "5148/5148 [==============================] - 60s 12ms/sample - loss: 1.6800 - accuracy: 0.6388 - binary_accuracy: 0.9963 - recall: 0.5650 - precision: 0.9447 - val_loss: 1.3122 - val_accuracy: 0.7082 - val_binary_accuracy: 0.9970 - val_recall: 0.6386 - val_precision: 0.9724\n",
      "Epoch 3/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 1.4678 - accuracy: 0.6691 - binary_accuracy: 0.9966 - recall: 0.5871 - precision: 0.9590 - val_loss: 1.1988 - val_accuracy: 0.7395 - val_binary_accuracy: 0.9971 - val_recall: 0.6718 - val_precision: 0.9383\n",
      "Epoch 4/70\n",
      "5148/5148 [==============================] - 42s 8ms/sample - loss: 1.2712 - accuracy: 0.7127 - binary_accuracy: 0.9967 - recall: 0.6077 - precision: 0.9623 - val_loss: 1.0717 - val_accuracy: 0.7597 - val_binary_accuracy: 0.9972 - val_recall: 0.6515 - val_precision: 0.9842\n",
      "Epoch 5/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 1.0862 - accuracy: 0.7502 - binary_accuracy: 0.9970 - recall: 0.6410 - precision: 0.9641 - val_loss: 0.8692 - val_accuracy: 0.8013 - val_binary_accuracy: 0.9976 - val_recall: 0.7122 - val_precision: 0.9782\n",
      "Epoch 6/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.8535 - accuracy: 0.8018 - binary_accuracy: 0.9975 - recall: 0.7064 - precision: 0.9650 - val_loss: 0.7793 - val_accuracy: 0.8195 - val_binary_accuracy: 0.9978 - val_recall: 0.7546 - val_precision: 0.9560\n",
      "Epoch 7/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.7036 - accuracy: 0.8323 - binary_accuracy: 0.9978 - recall: 0.7506 - precision: 0.9602 - val_loss: 0.6953 - val_accuracy: 0.8413 - val_binary_accuracy: 0.9979 - val_recall: 0.7880 - val_precision: 0.9425\n",
      "Epoch 8/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.5986 - accuracy: 0.8561 - binary_accuracy: 0.9980 - recall: 0.7817 - precision: 0.9602 - val_loss: 0.6535 - val_accuracy: 0.8479 - val_binary_accuracy: 0.9981 - val_recall: 0.8086 - val_precision: 0.9469\n",
      "Epoch 9/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.5058 - accuracy: 0.8762 - binary_accuracy: 0.9983 - recall: 0.8114 - precision: 0.9628 - val_loss: 0.6334 - val_accuracy: 0.8577 - val_binary_accuracy: 0.9982 - val_recall: 0.8201 - val_precision: 0.9399\n",
      "Epoch 10/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.4366 - accuracy: 0.8908 - binary_accuracy: 0.9985 - recall: 0.8336 - precision: 0.9651 - val_loss: 0.6139 - val_accuracy: 0.8605 - val_binary_accuracy: 0.9982 - val_recall: 0.8252 - val_precision: 0.9439\n",
      "Epoch 11/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.3841 - accuracy: 0.9023 - binary_accuracy: 0.9986 - recall: 0.8518 - precision: 0.9660 - val_loss: 0.5984 - val_accuracy: 0.8636 - val_binary_accuracy: 0.9982 - val_recall: 0.8234 - val_precision: 0.9438\n",
      "Epoch 12/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.3328 - accuracy: 0.9153 - binary_accuracy: 0.9988 - recall: 0.8674 - precision: 0.9696 - val_loss: 0.6003 - val_accuracy: 0.8679 - val_binary_accuracy: 0.9983 - val_recall: 0.8406 - val_precision: 0.9336\n",
      "Epoch 13/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.2751 - accuracy: 0.9303 - binary_accuracy: 0.9990 - recall: 0.8894 - precision: 0.9748 - val_loss: 0.6172 - val_accuracy: 0.8674 - val_binary_accuracy: 0.9983 - val_recall: 0.8433 - val_precision: 0.9291\n",
      "Epoch 14/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.2338 - accuracy: 0.9409 - binary_accuracy: 0.9991 - recall: 0.9049 - precision: 0.9775 - val_loss: 0.6147 - val_accuracy: 0.8695 - val_binary_accuracy: 0.9982 - val_recall: 0.8421 - val_precision: 0.9268\n",
      "Epoch 15/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.2047 - accuracy: 0.9491 - binary_accuracy: 0.9992 - recall: 0.9165 - precision: 0.9794 - val_loss: 0.6469 - val_accuracy: 0.8721 - val_binary_accuracy: 0.9982 - val_recall: 0.8529 - val_precision: 0.9199\n",
      "Epoch 16/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.1781 - accuracy: 0.9569 - binary_accuracy: 0.9993 - recall: 0.9276 - precision: 0.9819 - val_loss: 0.6209 - val_accuracy: 0.8736 - val_binary_accuracy: 0.9983 - val_recall: 0.8560 - val_precision: 0.9186\n",
      "Epoch 17/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.1651 - accuracy: 0.9599 - binary_accuracy: 0.9993 - recall: 0.9338 - precision: 0.9809 - val_loss: 0.6316 - val_accuracy: 0.8700 - val_binary_accuracy: 0.9983 - val_recall: 0.8513 - val_precision: 0.9216\n",
      "Epoch 18/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.1319 - accuracy: 0.9709 - binary_accuracy: 0.9995 - recall: 0.9494 - precision: 0.9862 - val_loss: 0.6411 - val_accuracy: 0.8731 - val_binary_accuracy: 0.9982 - val_recall: 0.8543 - val_precision: 0.9146\n",
      "Epoch 19/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.1105 - accuracy: 0.9764 - binary_accuracy: 0.9996 - recall: 0.9587 - precision: 0.9890 - val_loss: 0.6811 - val_accuracy: 0.8715 - val_binary_accuracy: 0.9982 - val_recall: 0.8562 - val_precision: 0.9086\n",
      "Epoch 20/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0930 - accuracy: 0.9808 - binary_accuracy: 0.9997 - recall: 0.9679 - precision: 0.9906 - val_loss: 0.6785 - val_accuracy: 0.8736 - val_binary_accuracy: 0.9982 - val_recall: 0.8561 - val_precision: 0.9117\n",
      "Epoch 21/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.0917 - accuracy: 0.9808 - binary_accuracy: 0.9997 - recall: 0.9679 - precision: 0.9896 - val_loss: 0.7068 - val_accuracy: 0.8741 - val_binary_accuracy: 0.9982 - val_recall: 0.8600 - val_precision: 0.9084\n",
      "Epoch 22/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0733 - accuracy: 0.9866 - binary_accuracy: 0.9998 - recall: 0.9766 - precision: 0.9927 - val_loss: 0.7381 - val_accuracy: 0.8700 - val_binary_accuracy: 0.9982 - val_recall: 0.8609 - val_precision: 0.9045\n",
      "Epoch 23/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.0620 - accuracy: 0.9892 - binary_accuracy: 0.9998 - recall: 0.9815 - precision: 0.9939 - val_loss: 0.7228 - val_accuracy: 0.8767 - val_binary_accuracy: 0.9983 - val_recall: 0.8685 - val_precision: 0.9086\n",
      "Epoch 24/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0451 - accuracy: 0.9935 - binary_accuracy: 0.9999 - recall: 0.9894 - precision: 0.9966 - val_loss: 0.7334 - val_accuracy: 0.8738 - val_binary_accuracy: 0.9982 - val_recall: 0.8632 - val_precision: 0.9065\n",
      "Epoch 25/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0368 - accuracy: 0.9954 - binary_accuracy: 0.9999 - recall: 0.9925 - precision: 0.9975 - val_loss: 0.7535 - val_accuracy: 0.8741 - val_binary_accuracy: 0.9982 - val_recall: 0.8649 - val_precision: 0.9038\n",
      "Epoch 26/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.0295 - accuracy: 0.9968 - binary_accuracy: 0.9999 - recall: 0.9948 - precision: 0.9979 - val_loss: 0.7544 - val_accuracy: 0.8751 - val_binary_accuracy: 0.9982 - val_recall: 0.8654 - val_precision: 0.9004\n",
      "Epoch 27/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0241 - accuracy: 0.9977 - binary_accuracy: 1.0000 - recall: 0.9965 - precision: 0.9985 - val_loss: 0.7732 - val_accuracy: 0.8731 - val_binary_accuracy: 0.9982 - val_recall: 0.8646 - val_precision: 0.8974\n",
      "Epoch 28/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.0201 - accuracy: 0.9981 - binary_accuracy: 1.0000 - recall: 0.9973 - precision: 0.9985 - val_loss: 0.7936 - val_accuracy: 0.8726 - val_binary_accuracy: 0.9982 - val_recall: 0.8654 - val_precision: 0.8995\n",
      "Epoch 29/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0181 - accuracy: 0.9984 - binary_accuracy: 1.0000 - recall: 0.9978 - precision: 0.9988 - val_loss: 0.8306 - val_accuracy: 0.8738 - val_binary_accuracy: 0.9982 - val_recall: 0.8670 - val_precision: 0.8951\n",
      "Epoch 30/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.0409 - accuracy: 0.9918 - binary_accuracy: 0.9999 - recall: 0.9890 - precision: 0.9938 - val_loss: 0.7714 - val_accuracy: 0.8723 - val_binary_accuracy: 0.9982 - val_recall: 0.8628 - val_precision: 0.8983\n",
      "Epoch 31/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.0518 - accuracy: 0.9896 - binary_accuracy: 0.9998 - recall: 0.9855 - precision: 0.9928 - val_loss: 0.8016 - val_accuracy: 0.8731 - val_binary_accuracy: 0.9982 - val_recall: 0.8677 - val_precision: 0.8988\n",
      "Epoch 32/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0355 - accuracy: 0.9940 - binary_accuracy: 0.9999 - recall: 0.9917 - precision: 0.9959 - val_loss: 0.7855 - val_accuracy: 0.8744 - val_binary_accuracy: 0.9982 - val_recall: 0.8687 - val_precision: 0.9017\n",
      "Epoch 33/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.0302 - accuracy: 0.9956 - binary_accuracy: 0.9999 - recall: 0.9939 - precision: 0.9966 - val_loss: 0.7549 - val_accuracy: 0.8785 - val_binary_accuracy: 0.9983 - val_recall: 0.8717 - val_precision: 0.9050\n",
      "Epoch 34/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0213 - accuracy: 0.9974 - binary_accuracy: 1.0000 - recall: 0.9965 - precision: 0.9979 - val_loss: 0.7772 - val_accuracy: 0.8785 - val_binary_accuracy: 0.9982 - val_recall: 0.8696 - val_precision: 0.8989\n",
      "Epoch 35/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.0167 - accuracy: 0.9981 - binary_accuracy: 1.0000 - recall: 0.9975 - precision: 0.9986 - val_loss: 0.7732 - val_accuracy: 0.8774 - val_binary_accuracy: 0.9982 - val_recall: 0.8686 - val_precision: 0.8991\n",
      "Epoch 36/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0150 - accuracy: 0.9982 - binary_accuracy: 1.0000 - recall: 0.9979 - precision: 0.9987 - val_loss: 0.8053 - val_accuracy: 0.8756 - val_binary_accuracy: 0.9982 - val_recall: 0.8712 - val_precision: 0.8966\n",
      "Epoch 37/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.0116 - accuracy: 0.9987 - binary_accuracy: 1.0000 - recall: 0.9985 - precision: 0.9990 - val_loss: 0.8012 - val_accuracy: 0.8785 - val_binary_accuracy: 0.9982 - val_recall: 0.8709 - val_precision: 0.8991\n",
      "Epoch 38/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0102 - accuracy: 0.9988 - binary_accuracy: 1.0000 - recall: 0.9986 - precision: 0.9991 - val_loss: 0.8342 - val_accuracy: 0.8751 - val_binary_accuracy: 0.9982 - val_recall: 0.8694 - val_precision: 0.8920\n",
      "Epoch 39/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0086 - accuracy: 0.9991 - binary_accuracy: 1.0000 - recall: 0.9989 - precision: 0.9993 - val_loss: 0.8214 - val_accuracy: 0.8769 - val_binary_accuracy: 0.9982 - val_recall: 0.8716 - val_precision: 0.8957\n",
      "Epoch 40/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0073 - accuracy: 0.9991 - binary_accuracy: 1.0000 - recall: 0.9989 - precision: 0.9992 - val_loss: 0.8268 - val_accuracy: 0.8772 - val_binary_accuracy: 0.9982 - val_recall: 0.8723 - val_precision: 0.8927\n",
      "Epoch 41/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0070 - accuracy: 0.9992 - binary_accuracy: 1.0000 - recall: 0.9991 - precision: 0.9993 - val_loss: 0.8323 - val_accuracy: 0.8774 - val_binary_accuracy: 0.9982 - val_recall: 0.8719 - val_precision: 0.8946\n",
      "Epoch 42/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.0060 - accuracy: 0.9993 - binary_accuracy: 1.0000 - recall: 0.9992 - precision: 0.9995 - val_loss: 0.8457 - val_accuracy: 0.8777 - val_binary_accuracy: 0.9982 - val_recall: 0.8729 - val_precision: 0.8933\n",
      "Epoch 43/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0054 - accuracy: 0.9993 - binary_accuracy: 1.0000 - recall: 0.9992 - precision: 0.9995 - val_loss: 0.8536 - val_accuracy: 0.8777 - val_binary_accuracy: 0.9982 - val_recall: 0.8710 - val_precision: 0.8939\n",
      "Epoch 44/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.0051 - accuracy: 0.9993 - binary_accuracy: 1.0000 - recall: 0.9992 - precision: 0.9995 - val_loss: 0.8697 - val_accuracy: 0.8772 - val_binary_accuracy: 0.9982 - val_recall: 0.8709 - val_precision: 0.8913\n",
      "Epoch 45/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0048 - accuracy: 0.9994 - binary_accuracy: 1.0000 - recall: 0.9992 - precision: 0.9995 - val_loss: 0.8545 - val_accuracy: 0.8779 - val_binary_accuracy: 0.9982 - val_recall: 0.8725 - val_precision: 0.8924\n",
      "Epoch 46/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0046 - accuracy: 0.9994 - binary_accuracy: 1.0000 - recall: 0.9994 - precision: 0.9995 - val_loss: 0.8806 - val_accuracy: 0.8767 - val_binary_accuracy: 0.9982 - val_recall: 0.8721 - val_precision: 0.8910\n",
      "Epoch 47/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0043 - accuracy: 0.9995 - binary_accuracy: 1.0000 - recall: 0.9994 - precision: 0.9996 - val_loss: 0.8916 - val_accuracy: 0.8779 - val_binary_accuracy: 0.9982 - val_recall: 0.8711 - val_precision: 0.8916\n",
      "Epoch 48/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0052 - accuracy: 0.9991 - binary_accuracy: 1.0000 - recall: 0.9991 - precision: 0.9993 - val_loss: 0.9600 - val_accuracy: 0.8687 - val_binary_accuracy: 0.9981 - val_recall: 0.8652 - val_precision: 0.8834\n",
      "Epoch 49/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.0564 - accuracy: 0.9851 - binary_accuracy: 0.9998 - recall: 0.9821 - precision: 0.9879 - val_loss: 0.8094 - val_accuracy: 0.8708 - val_binary_accuracy: 0.9981 - val_recall: 0.8620 - val_precision: 0.8939\n",
      "Epoch 50/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0631 - accuracy: 0.9841 - binary_accuracy: 0.9998 - recall: 0.9801 - precision: 0.9882 - val_loss: 0.7488 - val_accuracy: 0.8695 - val_binary_accuracy: 0.9981 - val_recall: 0.8622 - val_precision: 0.8934\n",
      "Epoch 51/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.0332 - accuracy: 0.9926 - binary_accuracy: 0.9999 - recall: 0.9905 - precision: 0.9944 - val_loss: 0.7573 - val_accuracy: 0.8759 - val_binary_accuracy: 0.9982 - val_recall: 0.8684 - val_precision: 0.8995\n",
      "Epoch 52/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0152 - accuracy: 0.9978 - binary_accuracy: 1.0000 - recall: 0.9974 - precision: 0.9984 - val_loss: 0.7615 - val_accuracy: 0.8777 - val_binary_accuracy: 0.9982 - val_recall: 0.8688 - val_precision: 0.8987\n",
      "Epoch 53/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0092 - accuracy: 0.9988 - binary_accuracy: 1.0000 - recall: 0.9986 - precision: 0.9989 - val_loss: 0.7981 - val_accuracy: 0.8774 - val_binary_accuracy: 0.9982 - val_recall: 0.8711 - val_precision: 0.8962\n",
      "Epoch 54/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.0074 - accuracy: 0.9989 - binary_accuracy: 1.0000 - recall: 0.9989 - precision: 0.9991 - val_loss: 0.8041 - val_accuracy: 0.8787 - val_binary_accuracy: 0.9982 - val_recall: 0.8739 - val_precision: 0.8972\n",
      "Epoch 55/70\n",
      "5148/5148 [==============================] - 42s 8ms/sample - loss: 0.0061 - accuracy: 0.9993 - binary_accuracy: 1.0000 - recall: 0.9991 - precision: 0.9994 - val_loss: 0.8052 - val_accuracy: 0.8792 - val_binary_accuracy: 0.9982 - val_recall: 0.8749 - val_precision: 0.8942\n",
      "Epoch 56/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.0048 - accuracy: 0.9994 - binary_accuracy: 1.0000 - recall: 0.9992 - precision: 0.9994 - val_loss: 0.8074 - val_accuracy: 0.8808 - val_binary_accuracy: 0.9983 - val_recall: 0.8780 - val_precision: 0.9002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0041 - accuracy: 0.9995 - binary_accuracy: 1.0000 - recall: 0.9994 - precision: 0.9996 - val_loss: 0.8270 - val_accuracy: 0.8797 - val_binary_accuracy: 0.9982 - val_recall: 0.8743 - val_precision: 0.8971\n",
      "Epoch 58/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.0037 - accuracy: 0.9995 - binary_accuracy: 1.0000 - recall: 0.9993 - precision: 0.9996 - val_loss: 0.8403 - val_accuracy: 0.8785 - val_binary_accuracy: 0.9982 - val_recall: 0.8741 - val_precision: 0.8956\n",
      "Epoch 59/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.0037 - accuracy: 0.9995 - binary_accuracy: 1.0000 - recall: 0.9994 - precision: 0.9995 - val_loss: 0.8387 - val_accuracy: 0.8779 - val_binary_accuracy: 0.9982 - val_recall: 0.8747 - val_precision: 0.8949\n",
      "Epoch 60/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0032 - accuracy: 0.9996 - binary_accuracy: 1.0000 - recall: 0.9995 - precision: 0.9997 - val_loss: 0.8560 - val_accuracy: 0.8790 - val_binary_accuracy: 0.9982 - val_recall: 0.8755 - val_precision: 0.8943\n",
      "Epoch 61/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.0032 - accuracy: 0.9995 - binary_accuracy: 1.0000 - recall: 0.9994 - precision: 0.9996 - val_loss: 0.8651 - val_accuracy: 0.8790 - val_binary_accuracy: 0.9982 - val_recall: 0.8733 - val_precision: 0.8937\n",
      "Epoch 62/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0030 - accuracy: 0.9996 - binary_accuracy: 1.0000 - recall: 0.9995 - precision: 0.9996 - val_loss: 0.8663 - val_accuracy: 0.8759 - val_binary_accuracy: 0.9982 - val_recall: 0.8704 - val_precision: 0.8914\n",
      "Epoch 63/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.0029 - accuracy: 0.9995 - binary_accuracy: 1.0000 - recall: 0.9995 - precision: 0.9996 - val_loss: 0.8806 - val_accuracy: 0.8759 - val_binary_accuracy: 0.9982 - val_recall: 0.8715 - val_precision: 0.8894\n",
      "Epoch 64/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0027 - accuracy: 0.9996 - binary_accuracy: 1.0000 - recall: 0.9995 - precision: 0.9996 - val_loss: 0.8869 - val_accuracy: 0.8774 - val_binary_accuracy: 0.9982 - val_recall: 0.8731 - val_precision: 0.8898\n",
      "Epoch 65/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0027 - accuracy: 0.9995 - binary_accuracy: 1.0000 - recall: 0.9994 - precision: 0.9995 - val_loss: 0.8859 - val_accuracy: 0.8774 - val_binary_accuracy: 0.9982 - val_recall: 0.8735 - val_precision: 0.8918\n",
      "Epoch 66/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0043 - accuracy: 0.9992 - binary_accuracy: 1.0000 - recall: 0.9991 - precision: 0.9994 - val_loss: 0.8762 - val_accuracy: 0.8728 - val_binary_accuracy: 0.9981 - val_recall: 0.8653 - val_precision: 0.8881\n",
      "Epoch 67/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0240 - accuracy: 0.9941 - binary_accuracy: 0.9999 - recall: 0.9932 - precision: 0.9951 - val_loss: 0.8424 - val_accuracy: 0.8718 - val_binary_accuracy: 0.9982 - val_recall: 0.8666 - val_precision: 0.8926\n",
      "Epoch 68/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.0326 - accuracy: 0.9919 - binary_accuracy: 0.9999 - recall: 0.9906 - precision: 0.9936 - val_loss: 0.8198 - val_accuracy: 0.8715 - val_binary_accuracy: 0.9981 - val_recall: 0.8675 - val_precision: 0.8930\n",
      "Epoch 69/70\n",
      "5148/5148 [==============================] - 40s 8ms/sample - loss: 0.0226 - accuracy: 0.9950 - binary_accuracy: 0.9999 - recall: 0.9940 - precision: 0.9960 - val_loss: 0.8006 - val_accuracy: 0.8823 - val_binary_accuracy: 0.9983 - val_recall: 0.8763 - val_precision: 0.8995\n",
      "Epoch 70/70\n",
      "5148/5148 [==============================] - 39s 8ms/sample - loss: 0.0119 - accuracy: 0.9978 - binary_accuracy: 1.0000 - recall: 0.9975 - precision: 0.9982 - val_loss: 0.8483 - val_accuracy: 0.8738 - val_binary_accuracy: 0.9982 - val_recall: 0.8705 - val_precision: 0.8948\n"
     ]
    }
   ],
   "source": [
    "new_experiment.run_experiment(one_hot_reports_train, one_hot_mesh_shifted_train, one_hot_mesh_train, \n",
    "                              one_hot_reports_val, one_hot_mesh_shifted_val, one_hot_mesh_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_name = new_experiment.model_name\n",
    "model_output_dir = dir + 'trained_models/{}/'.format(model_name)\n",
    "new_experiment.save_weights_history(model_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results of specific experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'hiercnn2seq_att'\n",
    "model_output_dir = dir + 'trained_models/{}/'.format(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word encoder emb:  (None, 37, 1475)\n",
      "Word conv1 output:  (None, 37, 64)\n",
      "Word conv2 output:  (None, 37, 64)\n",
      "Word conv3 output:  (None, 37, 64)\n",
      "Word Concat:  (None, 37, 192)\n",
      "Max pool outputs:  (None, 37, 192)\n",
      "Phrase Concat:  (None, 37, 640)\n",
      "Encoder states:  (None, 640)\n",
      "Decoder outputs:  (None, 13, 640)\n",
      "Attention outputs:  (None, 13, 640)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from utils.cnn_textsum_models import HierCNN2SeqAtt\n",
    "\n",
    "model_name = 'hiercnn2seq_att'\n",
    "embedding_dim = 256\n",
    "word_conv_dim = 64\n",
    "phrase_conv_dim = 128\n",
    "conv1_kernel = 1\n",
    "conv2_kernel = 3\n",
    "conv3_kernel = 5\n",
    "epochs = 70\n",
    "\n",
    "param_fn = 'param_{}_wordconvdim_{}_phraseconvdim_{}_kernels_{}{}{}_epochs_{}.pkl'.format(model_name,\\\n",
    "word_conv_dim, phrase_conv_dim, conv1_kernel, conv2_kernel, conv3_kernel, epochs)\n",
    "params = pickle.load(open(model_output_dir + param_fn, 'rb'))\n",
    "\n",
    "old_experiment = HierCNN2SeqAtt(**params)\n",
    "old_experiment.build_model()\n",
    "old_experiment.load_weights_history(model_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode a one hot encoded string\n",
    "def one_hot_decode(encoded_seq):\n",
    "    return [np.argmax(vector) for vector in encoded_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cons_duplicates(seq):\n",
    "    newlst = [k for k, g in itertools.groupby(seq)]\n",
    "    return newlst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_start_end(seq, start_token='start', end_token='end'):\n",
    "    stripped_seq = []\n",
    "    for s in seq:\n",
    "        if s not in [start_token, end_token]:\n",
    "            stripped_seq.append(s)\n",
    "    stripped_seq = remove_cons_duplicates(stripped_seq)\n",
    "    return stripped_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(experiment, source, max_seq_len, id_to_mesh, start_token='start', end_token='end'):\n",
    "    # encode source\n",
    "    enc_outs, h, c = experiment.encoder_model.predict(source)\n",
    "    enc_state = [h,c]\n",
    "    dec_state = enc_state\n",
    "\n",
    "    # start of sequence input\n",
    "    in_text = [start_token]\n",
    "\n",
    "    # integer encoder\n",
    "    in_seq_ids = dpt.mesh_to_vectors([in_text], dicts_dir=data_dir+'dicts/', \n",
    "                   load_dicts=True, save=False)\n",
    "    # one-hot encode\n",
    "    in_seq_onehot = dpt.one_hot_sequence(in_seq_ids, mesh_vocab_length)\n",
    "    in_seq_onehot = np.array(in_seq_onehot)\n",
    "    in_seq_onehot = in_seq_onehot.reshape(1, 1, in_seq_onehot.shape[-1])\n",
    "    target_seq = in_seq_onehot\n",
    "    \n",
    "    # collect predictions\n",
    "    output = []\n",
    "    attention_weights = []\n",
    "    max_att = []\n",
    "    max_att2 = []\n",
    "    for t in range(max_seq_len):\n",
    "        dec_out, attention, h, c  = experiment.decoder_model.predict([enc_outs] + dec_state + [target_seq])\n",
    "        dec_state = [h,c]\n",
    "        # store prediction\n",
    "        output.append(dec_out[0,0,:])\n",
    "        dec_ind = np.argmax(dec_out, axis=-1)[0, 0]\n",
    "\n",
    "        attention_weights.append((dec_ind, attention))\n",
    "        idx = np.argmax(attention, axis=-1)[0, 0]\n",
    "        max_att.append(np.argmax(attention, axis=-1)[0, 0])\n",
    "        attention[:,:,idx] = 0\n",
    "        max_att2.append(np.argmax(attention, axis=-1)[0, 0])\n",
    "        target_seq = dec_out\n",
    "\n",
    "    predicted_mesh_ids = one_hot_decode(output)\n",
    "    predicted_mesh = [id_to_mesh[idx] for idx in predicted_mesh_ids]\n",
    "    \n",
    "    return predicted_mesh, attention_weights, max_att, max_att2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed report:  interval cabg . sternotomy appear intact . stable , mild degenerative disc disease thoracic spine . visualized bony structures otherwise unremarkable appearance . atherosclerotic calcifications thoracic aorta . clear lungs . peripheral vascular disease\n",
      "True mesh caption:  atherosclerosis aorta_thoracic thoracic_vertebrae degenerative mild\n",
      "Predicted mesh caption:  atherosclerosis aorta_thoracic thoracic_vertebrae degenerative mild\n",
      "\n",
      "Attention word inputs 1:  ['calcifications', 'calcifications', 'thoracic', 'thoracic', 'disc', 'degenerative', 'lungs', 'lungs', 'disease', 'disease', 'disease', 'disease', 'disease']\n",
      "Attention word inputs 2:  ['thoracic', 'thoracic', 'calcifications', 'disc', 'disease', 'disc', '.', 'clear', 'lungs', 'lungs', 'lungs', 'lungs', 'lungs']\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1):\n",
    "    sample = val_df.sample(1, random_state=42)\n",
    "\n",
    "    true_mesh_caption = list(sample.all_mesh)[0]\n",
    "    sample_report = list(sample.pad_text_report)[0]\n",
    "\n",
    "    sample_report_ids = []\n",
    "    for token in sample_report:\n",
    "        if token in word_to_id.keys():\n",
    "            sample_report_ids.append(word_to_id[token])\n",
    "        else:\n",
    "            sample_report_ids.append(word_to_id[unknown_token])\n",
    "\n",
    "    sample_report_ids = np.array(sample_report_ids)\n",
    "    sample_report_ids = np.array(sample_report_ids).reshape(1, len(sample_report_ids))\n",
    "    one_hot_sample_report = dpt.one_hot_sequence(sample_report_ids, report_vocab_length)\n",
    "    predicted_mesh, attention_weights, max_att1, max_att2 = predict_sequence(old_experiment, \n",
    "                                  one_hot_sample_report, \n",
    "                                  max_mesh_length, \n",
    "                                  id_to_mesh,\n",
    "                                  start_token=start_token,\n",
    "                                  end_token=end_token)\n",
    "#     predicted_mesh = predict_sequence(old_experiment.inference_model, \n",
    "#                                   sample_report_ids, \n",
    "#                                   max_mesh_length, \n",
    "#                                   id_to_mesh,\n",
    "#                                   start_token, \n",
    "#                                   end_token)\n",
    "#     predicted_mesh_ids = one_hot_decode(prediction)\n",
    "#     predicted_mesh = [id_to_mesh[idx] for idx in predicted_mesh_ids]\n",
    "    \n",
    "    sample_report_s = strip_start_end(sample_report)\n",
    "    predicted_mesh = strip_start_end(predicted_mesh)\n",
    "    print('')\n",
    "    print('Processed report: ', ' '.join(sample_report_s))\n",
    "    print('True mesh caption: ', ' '.join(true_mesh_caption))\n",
    "    print('Predicted mesh caption: ', ' '.join(predicted_mesh))\n",
    "    print('')\n",
    "    \n",
    "    att_words1 = [sample_report[k] for k in max_att1]\n",
    "    att_words2 = [sample_report[k] for k in max_att2]\n",
    "\n",
    "    print('Attention word inputs 1: ', att_words1)\n",
    "    print('Attention word inputs 2: ', att_words2)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate BLEU scores on all trian/val/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from progress.bar import Bar\n",
    "\n",
    "def evaluate_model(model, df, report_vocab_length):\n",
    "    actual, predicted = list(), list()\n",
    "    bleu1, bleu2, bleu3, bleu4 = list(), list(), list(), list()\n",
    "    bar = Bar('Processing', max=len(df))\n",
    "    \n",
    "    for _, sample in df.iterrows():\n",
    "        true_mesh_caption = sample.all_mesh\n",
    "        sample_report = sample.pad_text_report\n",
    "\n",
    "        sample_report_ids = []\n",
    "        for token in sample_report:\n",
    "            if token in word_to_id.keys():\n",
    "                sample_report_ids.append(word_to_id[token])\n",
    "            else:\n",
    "                sample_report_ids.append(word_to_id[unknown_token])\n",
    "\n",
    "        sample_report_ids = np.array(sample_report_ids).reshape(1, len(sample_report_ids))\n",
    "        one_hot_sample_report = dpt.one_hot_sequence(sample_report_ids, report_vocab_length)\n",
    "        predicted_mesh, _, _, _ = predict_sequence(model, \n",
    "                                  one_hot_sample_report, \n",
    "                                  max_mesh_length, \n",
    "                                  id_to_mesh,\n",
    "                                  start_token=start_token,\n",
    "                                  end_token=end_token)\n",
    "\n",
    "        # sample_report = strip_start_end(sample_report)\n",
    "        yhat = strip_start_end(predicted_mesh)\n",
    "        reference = true_mesh_caption\n",
    "        \n",
    "        # calculate BLEU score\n",
    "        bleu1.append(sentence_bleu([reference], yhat, weights=(1.0, 0, 0, 0)))\n",
    "        bleu2.append(sentence_bleu([reference], yhat, weights=(0.5, 0.5, 0, 0)))\n",
    "        bleu3.append(sentence_bleu([reference], yhat, weights=(0.3, 0.3, 0.3, 0)))\n",
    "        bleu4.append(sentence_bleu([reference], yhat, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "    \n",
    "        # store actual and predicted\n",
    "        actual.append(reference)\n",
    "        predicted.append(yhat)\n",
    "        \n",
    "        bar.next()\n",
    "        \n",
    "    print('BLEU1: ', np.mean(bleu1)*100)\n",
    "    print('BLEU2: ', np.mean(bleu2)*100)\n",
    "    print('BLEU3: ', np.mean(bleu3)*100)\n",
    "    print('BLEU4: ', np.mean(bleu4)*100)\n",
    "    bar.finish()\n",
    "    return actual, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_actual, train_predicted = evaluate_model(old_experiment, train_df.sample(2000), report_vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/medic02/users/ag6516/python3env/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/vol/medic02/users/ag6516/python3env/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/vol/medic02/users/ag6516/python3env/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU1:  74.43612935115107\n",
      "BLEU2:  33.67999060580746\n",
      "BLEU3:  24.766583200228588\n",
      "BLEU4:  13.831715644175018\n"
     ]
    }
   ],
   "source": [
    "val_actual, val_predicted = evaluate_model(old_experiment, val_df, report_vocab_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate ROUGE scores on all train/val/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rouge\n",
    "\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                       max_n=4,\n",
    "                       limit_length=True,\n",
    "                       length_limit=100,\n",
    "                       length_limit_type='words',\n",
    "                       apply_avg='Avg',\n",
    "                       apply_best='Best',\n",
    "                       alpha=0.5, # Default F1_score\n",
    "                       weight_factor=1.2,\n",
    "                       stemming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_results(p, r, f):\n",
    "    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hypotheses = [' '.join(p) for p in train_predicted]\n",
    "train_references = [' '.join(a) for a in train_actual]\n",
    "\n",
    "scores = evaluator.get_scores(train_hypotheses, train_references)\n",
    "\n",
    "for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "    print(prepare_results(results['p'], results['r'], results['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\trouge-1:\tP: 87.11\tR: 78.56\tF1: 81.15\n",
      "\trouge-2:\tP: 43.83\tR: 36.72\tF1: 38.71\n",
      "\trouge-3:\tP: 32.26\tR: 27.58\tF1: 28.69\n",
      "\trouge-4:\tP: 20.85\tR: 18.17\tF1: 18.57\n",
      "\trouge-l:\tP: 86.67\tR: 79.45\tF1: 81.82\n",
      "\trouge-w:\tP: 82.74\tR: 63.26\tF1: 69.10\n"
     ]
    }
   ],
   "source": [
    "val_hypotheses = [' '.join(p) for p in val_predicted]\n",
    "val_references = [' '.join(a) for a in val_actual]\n",
    "\n",
    "scores = evaluator.get_scores(val_hypotheses, val_references)\n",
    "\n",
    "for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "    print(prepare_results(results['p'], results['r'], results['f']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_tools as pt\n",
    "\n",
    "pt.plot_history(old_ex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
