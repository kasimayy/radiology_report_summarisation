{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import json\n",
    "import sys\n",
    "# sys.path.append(\"..\")\n",
    "from utils import data_proc_tools as dpt\n",
    "from utils import plot_tools as pt\n",
    "from utils.custom_metrics import recall, precision, binary_accuracy\n",
    "from utils.custom_metrics import recall_np, precision_np, binary_accuracy_np, multilabel_confusion_matrix\n",
    "from utils.rnn_textsum_models import HierSeq2Seq\n",
    "import random\n",
    "random.seed(42)\n",
    "random_state=1000\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import pylab\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (8.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "dir = '/vol/medic02/users/ag6516/radiology_report_summarisation/'\n",
    "data_dir = dir + 'data/'\n",
    "\n",
    "aug = 'aug'\n",
    "\n",
    "model_output_dir = dir + 'trained_models/hierseq2seq/'\n",
    "\n",
    "train_df = pd.read_pickle(data_dir + 'train/{}_train.pkl'.format(aug))\n",
    "val_df = pd.read_pickle(data_dir + 'val/val.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare sequence data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>examid</th>\n",
       "      <th>report</th>\n",
       "      <th>all_mesh</th>\n",
       "      <th>single_mesh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CXR1000_IM-0003</td>\n",
       "      <td>[increased, opacity, within, right, upper, lobe, possible, mass, associated, area, atelectasis, focal, consolidation, ., cardiac, silhouette, within, normal, limits, ., opacity, left, midlung, overlying, posterior, left, 5th, rib, may, represent, focal, airspace, disease, ., increased, opacity, right, upper, lobe, associated, atelectasis, may, represent, focal, consolidation, mass, lesion, atelectasis, ., recommend, chest, ct, evaluation, ., opacity, overlying, left, 5th, rib, may, represent, focal, airspace, disease]</td>\n",
       "      <td>[opacity, lung, lingula, opacity, lung, upper_lobe, right, pulmonary_atelectasis, upper_lobe, right]</td>\n",
       "      <td>[opacity, lung, upper_lobe, right]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CXR1001_IM-0004</td>\n",
       "      <td>[interstitial, markings, diffusely, prominent, throughout, lungs, ., heart, size, normal, ., pulmonary, normal, ., diffuse, fibrosis]</td>\n",
       "      <td>[diffuse, markings, lung, bilateral, interstitial, diffuse, prominent]</td>\n",
       "      <td>[markings, lung, bilateral, interstitial, diffuse, prominent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CXR1002_IM-0004</td>\n",
       "      <td>[status, post, left, mastectomy, ., heart, size, normal, ., lungs, clear]</td>\n",
       "      <td>[left]</td>\n",
       "      <td>[left]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CXR1003_IM-0005</td>\n",
       "      <td>[heart, size, pulmonary, vascularity, appear, within, normal, limits, ., retrocardiac, soft, tissue, density, present, ., appears, air, within, suggest, represents, hiatal, hernia, ., vascular, calcification, noted, ., calcified, granuloma, seen, ., interval, development, bandlike, opacity, left, lung, base, ., may, represent, atelectasis, ., osteopenia, present, spine, ., retrocardiac, soft, tissue, density, ., appearance, suggests, hiatal, hernia, ., left, base, bandlike, opacity, ., appearance, suggests, atelectasis]</td>\n",
       "      <td>[bone_diseases_metabolic, spine, calcified_granuloma, calcinosis, blood_vessels, density, retrocardiac, opacity, lung, base, left]</td>\n",
       "      <td>[opacity, lung, base, left]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CXR1004_IM-0005</td>\n",
       "      <td>[heart, ,, pulmonary, mediastinum, within, normal, limits, ., aorta, tortuous, ectatic, ., degenerative, changes, acromioclavicular, joints, ., degenerative, changes, spine, ., ivc, identified]</td>\n",
       "      <td>[aorta, tortuous, catheters_indwelling, shoulder, bilateral, degenerative, spine, degenerative]</td>\n",
       "      <td>[shoulder, bilateral, degenerative]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            examid  \\\n",
       "0  CXR1000_IM-0003   \n",
       "1  CXR1001_IM-0004   \n",
       "2  CXR1002_IM-0004   \n",
       "3  CXR1003_IM-0005   \n",
       "4  CXR1004_IM-0005   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          report  \\\n",
       "0  [increased, opacity, within, right, upper, lobe, possible, mass, associated, area, atelectasis, focal, consolidation, ., cardiac, silhouette, within, normal, limits, ., opacity, left, midlung, overlying, posterior, left, 5th, rib, may, represent, focal, airspace, disease, ., increased, opacity, right, upper, lobe, associated, atelectasis, may, represent, focal, consolidation, mass, lesion, atelectasis, ., recommend, chest, ct, evaluation, ., opacity, overlying, left, 5th, rib, may, represent, focal, airspace, disease]     \n",
       "1  [interstitial, markings, diffusely, prominent, throughout, lungs, ., heart, size, normal, ., pulmonary, normal, ., diffuse, fibrosis]                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "2  [status, post, left, mastectomy, ., heart, size, normal, ., lungs, clear]                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "3  [heart, size, pulmonary, vascularity, appear, within, normal, limits, ., retrocardiac, soft, tissue, density, present, ., appears, air, within, suggest, represents, hiatal, hernia, ., vascular, calcification, noted, ., calcified, granuloma, seen, ., interval, development, bandlike, opacity, left, lung, base, ., may, represent, atelectasis, ., osteopenia, present, spine, ., retrocardiac, soft, tissue, density, ., appearance, suggests, hiatal, hernia, ., left, base, bandlike, opacity, ., appearance, suggests, atelectasis]   \n",
       "4  [heart, ,, pulmonary, mediastinum, within, normal, limits, ., aorta, tortuous, ectatic, ., degenerative, changes, acromioclavicular, joints, ., degenerative, changes, spine, ., ivc, identified]                                                                                                                                                                                                                                                                                                                                               \n",
       "\n",
       "                                                                                                                             all_mesh  \\\n",
       "0  [opacity, lung, lingula, opacity, lung, upper_lobe, right, pulmonary_atelectasis, upper_lobe, right]                                 \n",
       "1  [diffuse, markings, lung, bilateral, interstitial, diffuse, prominent]                                                               \n",
       "2  [left]                                                                                                                               \n",
       "3  [bone_diseases_metabolic, spine, calcified_granuloma, calcinosis, blood_vessels, density, retrocardiac, opacity, lung, base, left]   \n",
       "4  [aorta, tortuous, catheters_indwelling, shoulder, bilateral, degenerative, spine, degenerative]                                      \n",
       "\n",
       "                                                     single_mesh  \n",
       "0  [opacity, lung, upper_lobe, right]                             \n",
       "1  [markings, lung, bilateral, interstitial, diffuse, prominent]  \n",
       "2  [left]                                                         \n",
       "3  [opacity, lung, base, left]                                    \n",
       "4  [shoulder, bilateral, degenerative]                            "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepend and append start and end tokens to mesh captions and text reports\n",
    "start_token = 'start'\n",
    "end_token = 'end'\n",
    "unknown_token = '**unknown**'\n",
    "max_mesh_length = 13 # avg. + 1std. + start + end\n",
    "max_num_words = 11 # avg. + 1std. + start + end, max number of words per sentence\n",
    "max_num_sentences = 6 # avg. + 1std. + start + end, max number of sentences per report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pad_report(report, max_num_sentences, max_num_words, start_token, end_token):\n",
    "    pad_seq = [end_token for x in range(max_num_words)]\n",
    "    r = ' '.join(report)\n",
    "    sentences = r.split(' . ')\n",
    "    padded_sentences = []\n",
    "    for sen in sentences:\n",
    "        words = sen.split(' ')\n",
    "        padded_sentence = dpt.pad_sequence(words, max_num_words, start_token, end_token)\n",
    "        padded_sentences.append(padded_sentence)\n",
    "    if len(padded_sentences) >= max_num_sentences:\n",
    "        padded_sentences = padded_sentences[:max_num_sentences]\n",
    "    else:\n",
    "        while len(padded_sentences) < max_num_sentences:\n",
    "            padded_sentences.append(pad_seq)\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['pad_mesh_caption'] = train_df.all_mesh.apply(lambda x: dpt.pad_sequence(x, max_mesh_length, start_token, end_token))\n",
    "train_df['pad_text_report'] = train_df.report.apply(lambda x: split_pad_report(x, max_num_sentences, max_num_words, start_token, end_token))\n",
    "\n",
    "val_df['pad_mesh_caption'] = val_df.all_mesh.apply(lambda x: dpt.pad_sequence(x, max_mesh_length, start_token, end_token))\n",
    "val_df['pad_text_report'] = val_df.report.apply(lambda x: split_pad_report(x, max_num_sentences, max_num_words, start_token, end_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorise text reports and mesh captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mesh = list(train_df.pad_mesh_caption)\n",
    "train_reports = list(train_df.pad_text_report)\n",
    "\n",
    "# vectorize train mesh captions\n",
    "dpt.mesh_to_vectors(train_mesh, dicts_dir=data_dir+'dicts/', \n",
    "                    load_dicts=True, save=True, \n",
    "                    output_dir=data_dir+'train/')\n",
    "\n",
    "# vectorise train reports\n",
    "vec_reports = []\n",
    "for report in train_reports:\n",
    "    vec = dpt.reports_to_vectors(report, \n",
    "                                 dicts_dir=data_dir+'dicts/', \n",
    "                                 load_dicts=True, \n",
    "                                 output_dir=data_dir+'train/')\n",
    "    vec_reports.append(vec)\n",
    "vec_reports = np.array(vec_reports)\n",
    "np.save(data_dir+'train/' + 'sent_token_ids_array.npy', vec_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_reports = list(val_df.pad_text_report)\n",
    "val_mesh = list(val_df.pad_mesh_caption)\n",
    "\n",
    "# vectorise val mesh using the same dict as created for train\n",
    "dpt.mesh_to_vectors(val_mesh, dicts_dir=data_dir+'dicts/', \n",
    "                    load_dicts=True, save=True, \n",
    "                    output_dir=data_dir+'val/')\n",
    "\n",
    "# vectorise val reports\n",
    "vec_reports = []\n",
    "for report in val_reports:\n",
    "    vec = dpt.reports_to_vectors(report, \n",
    "                                 dicts_dir=data_dir+'dicts/', \n",
    "                                 load_dicts=True, \n",
    "                                 output_dir=data_dir+'train/')\n",
    "    vec_reports.append(vec)\n",
    "vec_reports = np.array(vec_reports)\n",
    "np.save(data_dir+'val/' + 'sent_token_ids_array.npy', vec_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id, id_to_word = dpt.load_report_dicts(data_dir+'dicts/')\n",
    "mesh_to_id, id_to_mesh = dpt.load_mesh_dicts(data_dir+'dicts/')\n",
    "\n",
    "report_vocab_length = len(word_to_id)\n",
    "mesh_vocab_length = len(mesh_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1475, 128)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_vocab_length, mesh_vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays of indixes for input sentences, output entities and shifted output entities (t-1)\n",
    "train_token_ids_array = np.load(data_dir + 'train/sent_token_ids_array.npy')\n",
    "train_mesh_ids_array = np.load(data_dir + 'train/mesh_ids_array.npy')\n",
    "train_mesh_ids_array_shifted =[np.concatenate((mesh_to_id[start_token], t[:-1]), axis=None) for t in train_mesh_ids_array]\n",
    "train_mesh_ids_array_shifted = np.asarray(train_mesh_ids_array_shifted)\n",
    "\n",
    "val_token_ids_array = np.load(data_dir + 'val/sent_token_ids_array.npy')\n",
    "val_mesh_ids_array = np.load(data_dir + 'val/mesh_ids_array.npy')\n",
    "val_mesh_ids_array_shifted = [np.concatenate((mesh_to_id[start_token], t[:-1]), axis=None) for t in val_mesh_ids_array]\n",
    "val_mesh_ids_array_shifted = np.asarray(val_mesh_ids_array_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot-encode\n",
    "one_hot_reports_train = []\n",
    "for report in train_token_ids_array:\n",
    "    one_hot_reports_train.append(dpt.one_hot_sequence(report, report_vocab_length))\n",
    "one_hot_reports_train = np.array(one_hot_reports_train)\n",
    "#dpt.one_hot_sequence(train_token_ids_array, report_vocab_length)\n",
    "one_hot_mesh_train = dpt.one_hot_sequence(train_mesh_ids_array, mesh_vocab_length)\n",
    "one_hot_mesh_shifted_train = dpt.one_hot_sequence(train_mesh_ids_array_shifted, mesh_vocab_length)\n",
    "\n",
    "one_hot_reports_val = []\n",
    "for report in val_token_ids_array:\n",
    "    one_hot_reports_val.append(dpt.one_hot_sequence(report, report_vocab_length))\n",
    "one_hot_reports_train = np.array(one_hot_reports_train)\n",
    "one_hot_mesh_val = dpt.one_hot_sequence(val_mesh_ids_array, mesh_vocab_length)\n",
    "one_hot_mesh_shifted_val = dpt.one_hot_sequence(val_mesh_ids_array_shifted, mesh_vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5148, 6, 11, 1475), (5148, 13, 128), (5148, 13, 128))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_reports_train.shape, one_hot_mesh_train.shape, one_hot_mesh_shifted_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Seq-to-Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 6, 11)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 6, 512)       4658176     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, None, 128)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 512), (None, 2099200     time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, None, 512),  1312768     input_3[0][0]                    \n",
      "                                                                 lstm_1[0][1]                     \n",
      "                                                                 lstm_1[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 128)    65664       lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 8,135,808\n",
      "Trainable params: 8,135,808\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from utils.custom_metrics import recall, precision, binary_accuracy\n",
    "from utils.rnn_textsum_models import HierSeq2Seq\n",
    "\n",
    "input_dim = len(word_to_id)\n",
    "output_dim = len(mesh_to_id)\n",
    "hidden_dim = 512\n",
    "encoder_emb_dim = 1024\n",
    "input_word_seq_length = max_num_words\n",
    "input_sent_seq_length = max_num_sentences\n",
    "output_seq_length = max_mesh_length\n",
    "epochs = 20\n",
    "optimizer = 'adam'\n",
    "loss='categorical_crossentropy'\n",
    "batch_size = 128\n",
    "\n",
    "new_experiment = HierSeq2Seq(epochs=epochs,\n",
    "                               metrics=['accuracy', binary_accuracy,recall,precision],\n",
    "                               optimizer=optimizer,\n",
    "                               loss=loss,\n",
    "                               batch_size=batch_size, \n",
    "                               input_dim=input_dim,\n",
    "                               output_dim=output_dim,\n",
    "                               hidden_dim=hidden_dim,\n",
    "                               encoder_emb_dim=encoder_emb_dim,\n",
    "                               input_word_seq_length=input_word_seq_length,\n",
    "                               input_sent_seq_length=input_sent_seq_length,\n",
    "                               output_seq_length=output_seq_length,\n",
    "                               verbose=True)\n",
    "new_experiment.build_model()\n",
    "new_experiment.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batch generators\n",
    "# train_batch_generator = dpt.batch_generator_seq2seq(train_token_ids_array, report_vocab_length, train_mesh_ids_array, \n",
    "#                                                    train_mesh_ids_array_shifted, mesh_vocab_length, batch_size)\n",
    "\n",
    "# val_batch_generator = dpt.batch_generator_seq2seq(val_token_ids_array, report_vocab_length, val_mesh_ids_array, \n",
    "#                                                    val_mesh_ids_array_shifted, mesh_vocab_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5148 samples, validate on 300 samples\n",
      "Epoch 1/20\n",
      "5148/5148 [==============================] - 52s 10ms/sample - loss: 2.1934 - accuracy: 0.5957 - binary_accuracy: 0.9950 - recall: 0.4778 - precision: 0.7551 - val_loss: 1.4694 - val_accuracy: 0.6977 - val_binary_accuracy: 0.9966 - val_recall: 0.6213 - val_precision: 0.9241\n",
      "Epoch 2/20\n",
      "5148/5148 [==============================] - 44s 9ms/sample - loss: 1.6599 - accuracy: 0.6438 - binary_accuracy: 0.9963 - recall: 0.5682 - precision: 0.9307 - val_loss: 1.3229 - val_accuracy: 0.7108 - val_binary_accuracy: 0.9970 - val_recall: 0.6572 - val_precision: 0.9376\n",
      "Epoch 3/20\n",
      "5148/5148 [==============================] - 44s 9ms/sample - loss: 1.5118 - accuracy: 0.6639 - binary_accuracy: 0.9965 - recall: 0.5894 - precision: 0.9465 - val_loss: 1.2167 - val_accuracy: 0.7290 - val_binary_accuracy: 0.9971 - val_recall: 0.6449 - val_precision: 0.9780\n",
      "Epoch 4/20\n",
      "5148/5148 [==============================] - 44s 9ms/sample - loss: 1.3564 - accuracy: 0.6909 - binary_accuracy: 0.9966 - recall: 0.5987 - precision: 0.9563 - val_loss: 1.0864 - val_accuracy: 0.7467 - val_binary_accuracy: 0.9972 - val_recall: 0.6835 - val_precision: 0.9502\n",
      "Epoch 5/20\n",
      "5148/5148 [==============================] - 44s 9ms/sample - loss: 1.1827 - accuracy: 0.7195 - binary_accuracy: 0.9968 - recall: 0.6238 - precision: 0.9593 - val_loss: 0.9854 - val_accuracy: 0.7651 - val_binary_accuracy: 0.9973 - val_recall: 0.6819 - val_precision: 0.9652\n",
      "Epoch 6/20\n",
      "5148/5148 [==============================] - 44s 9ms/sample - loss: 1.0488 - accuracy: 0.7455 - binary_accuracy: 0.9970 - recall: 0.6493 - precision: 0.9534 - val_loss: 0.8736 - val_accuracy: 0.7918 - val_binary_accuracy: 0.9975 - val_recall: 0.7069 - val_precision: 0.9671\n",
      "Epoch 7/20\n",
      "5148/5148 [==============================] - 44s 9ms/sample - loss: 0.9224 - accuracy: 0.7698 - binary_accuracy: 0.9972 - recall: 0.6781 - precision: 0.9520 - val_loss: 0.8090 - val_accuracy: 0.8023 - val_binary_accuracy: 0.9976 - val_recall: 0.7344 - val_precision: 0.9502\n",
      "Epoch 8/20\n",
      "5148/5148 [==============================] - 44s 9ms/sample - loss: 0.8240 - accuracy: 0.7892 - binary_accuracy: 0.9974 - recall: 0.7039 - precision: 0.9502 - val_loss: 0.7628 - val_accuracy: 0.8123 - val_binary_accuracy: 0.9978 - val_recall: 0.7428 - val_precision: 0.9652\n",
      "Epoch 9/20\n",
      "5148/5148 [==============================] - 44s 9ms/sample - loss: 0.7388 - accuracy: 0.8073 - binary_accuracy: 0.9976 - recall: 0.7261 - precision: 0.9523 - val_loss: 0.7285 - val_accuracy: 0.8208 - val_binary_accuracy: 0.9978 - val_recall: 0.7596 - val_precision: 0.9501\n",
      "Epoch 10/20\n",
      "5148/5148 [==============================] - 44s 9ms/sample - loss: 0.6655 - accuracy: 0.8238 - binary_accuracy: 0.9977 - recall: 0.7433 - precision: 0.9519 - val_loss: 0.7103 - val_accuracy: 0.8254 - val_binary_accuracy: 0.9978 - val_recall: 0.7760 - val_precision: 0.9396\n",
      "Epoch 11/20\n",
      "5148/5148 [==============================] - 44s 9ms/sample - loss: 0.6049 - accuracy: 0.8374 - binary_accuracy: 0.9978 - recall: 0.7606 - precision: 0.9533 - val_loss: 0.6825 - val_accuracy: 0.8287 - val_binary_accuracy: 0.9978 - val_recall: 0.7802 - val_precision: 0.9335\n",
      "Epoch 12/20\n",
      "5148/5148 [==============================] - 44s 8ms/sample - loss: 0.5438 - accuracy: 0.8531 - binary_accuracy: 0.9980 - recall: 0.7793 - precision: 0.9537 - val_loss: 0.6580 - val_accuracy: 0.8392 - val_binary_accuracy: 0.9979 - val_recall: 0.7946 - val_precision: 0.9353\n",
      "Epoch 13/20\n",
      "5148/5148 [==============================] - 44s 9ms/sample - loss: 0.4816 - accuracy: 0.8701 - binary_accuracy: 0.9982 - recall: 0.8010 - precision: 0.9562 - val_loss: 0.6517 - val_accuracy: 0.8423 - val_binary_accuracy: 0.9979 - val_recall: 0.7940 - val_precision: 0.9374\n",
      "Epoch 14/20\n",
      "5148/5148 [==============================] - 44s 9ms/sample - loss: 0.4273 - accuracy: 0.8841 - binary_accuracy: 0.9983 - recall: 0.8192 - precision: 0.9606 - val_loss: 0.6526 - val_accuracy: 0.8387 - val_binary_accuracy: 0.9980 - val_recall: 0.7972 - val_precision: 0.9316\n",
      "Epoch 15/20\n",
      "5148/5148 [==============================] - 44s 8ms/sample - loss: 0.3679 - accuracy: 0.9000 - binary_accuracy: 0.9985 - recall: 0.8389 - precision: 0.9646 - val_loss: 0.6491 - val_accuracy: 0.8421 - val_binary_accuracy: 0.9979 - val_recall: 0.8040 - val_precision: 0.9224\n",
      "Epoch 16/20\n",
      "5148/5148 [==============================] - 44s 9ms/sample - loss: 0.3206 - accuracy: 0.9147 - binary_accuracy: 0.9987 - recall: 0.8572 - precision: 0.9687 - val_loss: 0.6378 - val_accuracy: 0.8482 - val_binary_accuracy: 0.9980 - val_recall: 0.8172 - val_precision: 0.9176\n",
      "Epoch 17/20\n",
      "5148/5148 [==============================] - 44s 9ms/sample - loss: 0.2707 - accuracy: 0.9292 - binary_accuracy: 0.9989 - recall: 0.8794 - precision: 0.9736 - val_loss: 0.6660 - val_accuracy: 0.8423 - val_binary_accuracy: 0.9979 - val_recall: 0.8141 - val_precision: 0.9094\n",
      "Epoch 18/20\n",
      "5148/5148 [==============================] - 44s 9ms/sample - loss: 0.2286 - accuracy: 0.9424 - binary_accuracy: 0.9990 - recall: 0.8971 - precision: 0.9769 - val_loss: 0.6666 - val_accuracy: 0.8469 - val_binary_accuracy: 0.9980 - val_recall: 0.8201 - val_precision: 0.9090\n",
      "Epoch 19/20\n",
      "5148/5148 [==============================] - 44s 9ms/sample - loss: 0.1885 - accuracy: 0.9554 - binary_accuracy: 0.9992 - recall: 0.9167 - precision: 0.9817 - val_loss: 0.6587 - val_accuracy: 0.8479 - val_binary_accuracy: 0.9980 - val_recall: 0.8220 - val_precision: 0.9083\n",
      "Epoch 20/20\n",
      "5148/5148 [==============================] - 44s 9ms/sample - loss: 0.1544 - accuracy: 0.9659 - binary_accuracy: 0.9994 - recall: 0.9349 - precision: 0.9857 - val_loss: 0.6656 - val_accuracy: 0.8503 - val_binary_accuracy: 0.9980 - val_recall: 0.8245 - val_precision: 0.9092\n"
     ]
    }
   ],
   "source": [
    "new_experiment.run_experiment(train_token_ids_array, one_hot_mesh_shifted_train, one_hot_mesh_train, \n",
    "                              val_token_ids_array, one_hot_mesh_shifted_val, one_hot_mesh_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_name = 'hierseq2seq'\n",
    "model_output_dir = dir + 'trained_models/{}/'.format(model_name)\n",
    "new_experiment.save_weights_history(model_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results of specific experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'hierseq2seq'\n",
    "model_output_dir = dir + 'trained_models/{}/'.format(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_name = 'hierseq2seq'\n",
    "epochs = 20\n",
    "encoder_emb_dim = 1024\n",
    "decoder_emb_dim = 256\n",
    "hidden_dim = 512\n",
    "\n",
    "param_fn = 'param_encoderembdim_{}_hiddendim_{}_epochs_{}.pkl'.format(encoder_emb_dim,\n",
    "                                                                               hidden_dim,\n",
    "                                                                               epochs)\n",
    "params = pickle.load(open(model_output_dir + param_fn, 'rb'))\n",
    "\n",
    "old_experiment = HierSeq2Seq(**params)\n",
    "old_experiment.build_model()\n",
    "old_experiment.load_weights_history(model_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode a one hot encoded string\n",
    "def one_hot_decode(encoded_seq):\n",
    "    return [np.argmax(vector) for vector in encoded_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_start_end(seq, start_token='start', end_token='end'):\n",
    "    stripped_seq = []\n",
    "    for s in seq:\n",
    "        if s not in [start_token, end_token]:\n",
    "            stripped_seq.append(s)\n",
    "    return stripped_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence_att(experiment, source, max_seq_len, id_to_mesh, start_token='start', end_token='end'):\n",
    "    # encode source\n",
    "    enc_outs, h, c = experiment.encoder_model.predict(source)\n",
    "    enc_state = [h,c]\n",
    "    dec_state = enc_state\n",
    "\n",
    "    # start of sequence input\n",
    "    in_text = [start_token]\n",
    "\n",
    "    # integer encoder\n",
    "    in_seq_ids = dpt.mesh_to_vectors([in_text], dicts_dir=data_dir+'dicts/', \n",
    "                   load_dicts=True, save=False)\n",
    "    # one-hot encode\n",
    "    in_seq_onehot = dpt.one_hot_sequence(in_seq_ids, mesh_vocab_length)\n",
    "    in_seq_onehot = np.array(in_seq_onehot)\n",
    "    in_seq_onehot = in_seq_onehot.reshape(1, 1, in_seq_onehot.shape[-1])\n",
    "    target_seq = in_seq_onehot\n",
    "    \n",
    "    # collect predictions\n",
    "    output = []\n",
    "    attention_weights = []\n",
    "    max_att = []\n",
    "    for t in range(max_seq_len):\n",
    "        dec_out, attention, h, c  = experiment.decoder_model.predict([enc_outs] + dec_state + [target_seq])\n",
    "        dec_state = [h,c]\n",
    "        # store prediction\n",
    "        output.append(dec_out[0,0,:])\n",
    "        dec_ind = np.argmax(dec_out, axis=-1)[0, 0]\n",
    "\n",
    "        attention_weights.append((dec_ind, attention))\n",
    "        max_att.append(np.argmax(attention, axis=-1)[0, 0])\n",
    "        target_seq = dec_out\n",
    "\n",
    "    predicted_mesh_ids = one_hot_decode(output)\n",
    "    predicted_mesh = [id_to_mesh[idx] for idx in predicted_mesh_ids]\n",
    "    \n",
    "    return predicted_mesh, attention_weights, max_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(experiment, report_ids, max_seq_len, id_to_mesh, start_token='start', end_token='end'):\n",
    "    \n",
    "    in_text = []\n",
    "    for t in range(max_seq_len-1):\n",
    "        # pad input\n",
    "\n",
    "        in_seq = dpt.pad_sequence(in_text, max_seq_len, \n",
    "                                  start_token=start_token, \n",
    "                                  end_token=end_token)\n",
    "        #print(in_seq)\n",
    "        # integer encoder\n",
    "        in_seq_ids = dpt.mesh_to_vectors([in_seq], dicts_dir=data_dir+'dicts/', \n",
    "                       load_dicts=True, save=False)\n",
    "        #print(in_seq_ids.shape)\n",
    "        # one-hot encode\n",
    "        in_seq_onehot = dpt.one_hot_sequence(in_seq_ids, mesh_vocab_length)\n",
    "        #print(in_seq_onehot.shape)\n",
    "        \n",
    "        prediction = experiment.model.predict([report_ids, in_seq_onehot])\n",
    "        #print(prediction.shape)\n",
    "        \n",
    "        prediction = np.argmax(prediction,axis=2)\n",
    "        #print(prediction)\n",
    "        mesh = id_to_mesh[prediction[0][t+1]]\n",
    "        #print(mesh)\n",
    "        if mesh == end_token:\n",
    "            break\n",
    "        in_text.append(mesh)\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original report:  [['start', 'heart', 'size', 'cardiomediastinal', 'contours', 'normal', 'end', 'end', 'end', 'end', 'end'], ['start', 'low', 'lung', 'volumes', 'without', 'focal', 'airspace', 'opacity', ',', 'pleural', 'end'], ['start', 'multilevel', 'degenerative', 'changes', 'spine', 'end', 'end', 'end', 'end', 'end', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end']]\n",
      "True mesh caption:  ['lung', 'hypoinflation', 'spine', 'degenerative', 'multiple']\n",
      "Predicted mesh caption:  ['medical_device', 'medical_device', 'medical_device', 'medical_device']\n",
      "\n",
      "Original report:  [['start', 'heart', 'size', 'normal', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'central', 'left', 'midlung', 'granuloma', 'calcified', 'left', 'hilar', 'adenopathy', 'end', 'end'], ['start', 'bony', 'structures', 'appear', 'intact', 'end', 'end', 'end', 'end', 'end', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end']]\n",
      "True mesh caption:  ['calcinosis', 'lung', 'hilum', 'lymph_nodes', 'left', 'granuloma', 'lung', 'lingula']\n",
      "Predicted mesh caption:  ['medical_device', 'medical_device', 'medical_device', 'medical_device']\n",
      "\n",
      "Original report:  [['start', 'normal', 'heart', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'clear', 'lungs', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'trachea', 'midline', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'scoliosis', 'lower', 'thoracic', 'spine', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'degenerative', 'changes', 'thoracic', 'spine', 'end', 'end', 'end', 'end', 'end', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end']]\n",
      "True mesh caption:  ['scoliosis', 'thoracic_vertebrae', 'thoracic_vertebrae', 'degenerative']\n",
      "Predicted mesh caption:  ['medical_device', 'medical_device']\n",
      "\n",
      "Original report:  [['start', 'heart', 'enlarged', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'pulmonary', 'vascular', 'congestion', 'diffusely', 'increased', 'interstitial', 'mild', 'patchy', 'airspace', 'end'], ['start', 'distribution', 'pulmonary', 'edema', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'cardiomegaly', 'vascular', 'congestion', 'suspected', 'pulmonary', 'edema', 'end', 'end', 'end', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end']]\n",
      "True mesh caption:  ['cardiomegaly', 'opacity', 'lung', 'interstitial', 'diffuse', 'patchy', 'mild', 'pulmonary_congestion', 'pulmonary_edema']\n",
      "Predicted mesh caption:  ['medical_device', 'medical_device', 'medical_device', 'medical_device', 'medical_device', 'medical_device', 'medical_device']\n",
      "\n",
      "Original report:  [['start', 'cardiomediastinal', 'silhouette', 'within', 'normal', 'limits', 'size', 'contour', 'end', 'end', 'end'], ['start', 'lungs', 'normally', 'inflated', 'without', 'evidence', 'focal', 'airspace', 'disease', ',', 'end'], ['start', 'osseous', 'structures', 'within', 'normal', 'limits', 'patient', 'age', 'end', 'end', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end']]\n",
      "True mesh caption:  ['normal']\n",
      "Predicted mesh caption:  ['medical_device']\n",
      "\n",
      "Original report:  [['start', 'left', 'midlung', ',', 'basilar', 'streaky', 'opacity', 'end', 'end', 'end', 'end'], ['start', 'elevation', 'left', 'hemidiaphragm', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'small', 'calcified', 'cm', 'granuloma', 'adjacent', 'right', 'diaphragm', 'within', 'right', 'end'], ['start', 'cardiomediastinal', 'silhouette', 'within', 'normal', 'limits', 'end', 'end', 'end', 'end', 'end'], ['start', 'continued', 'clinical', 'concern', 'rib', 'fracture', 'dedicated', 'rib', 'films', 'will', 'end'], ['start', 'left', 'midlung', ',', 'left', 'basilar', 'streaky', 'opacity', 'may', 'represent', 'end']]\n",
      "True mesh caption:  ['calcified_granuloma', 'thorax', 'right', 'small', 'diaphragm', 'left', 'elevated', 'opacity', 'lung', 'base', 'left', 'streaky', 'opacity', 'lung', 'lingula', 'streaky']\n",
      "Predicted mesh caption:  ['medical_device', 'medical_device', 'medical_device', 'medical_device', 'medical_device', 'medical_device', 'medical_device', 'medical_device', 'medical_device']\n",
      "\n",
      "Original report:  [['start', 'cardiomediastinal', 'silhouette', 'within', 'normal', 'limits', 'appearance', 'end', 'end', 'end', 'end'], ['start', 'trachea', 'midline', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'minimal', 'degenerative', 'changes', 'thoracic', 'spine', 'end', 'end', 'end', 'end', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end']]\n",
      "True mesh caption:  ['thoracic_vertebrae', 'degenerative', 'mild']\n",
      "Predicted mesh caption:  ['medical_device', 'medical_device']\n",
      "\n",
      "Original report:  [['start', 'lungs', 'clear', 'bilaterally', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'cardio', 'mediastinal', 'silhouette', 'unremarkable', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'visualized', 'osseous', 'structures', 'thorax', 'demonstrate', 'healed', ',', 'remote', 'bilateral', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end']]\n",
      "True mesh caption:  ['fractures_bone', 'ribs', 'bilateral', 'multiple', 'healed']\n",
      "Predicted mesh caption:  ['medical_device']\n",
      "\n",
      "Original report:  [['start', 'heart', 'top', 'normal', 'size', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'mediastinum', 'unremarkable', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'lungs', 'clear', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end']]\n",
      "True mesh caption:  ['normal']\n",
      "Predicted mesh caption:  ['medical_device']\n",
      "\n",
      "Original report:  [['start', 'cardiac', 'contours', 'normal', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'prominent', 'hilar', 'contours', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'lungs', 'clear', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'thoracic', 'spondylosis', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end']]\n",
      "True mesh caption:  ['lung', 'hilum', 'prominent', 'spondylosis', 'thoracic_vertebrae']\n",
      "Predicted mesh caption:  ['medical_device', 'medical_device', 'medical_device']\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    sample = val_df.sample(1)\n",
    "    true_mesh_caption = list(sample.all_mesh)[0]\n",
    "    sample_report = list(sample.pad_text_report)[0]\n",
    "    \n",
    "    sample_report_ids = []\n",
    "    for sent in sample_report:\n",
    "        sent_ids = []\n",
    "        for token in sent:\n",
    "            if token in word_to_id.keys():\n",
    "                sent_ids.append(word_to_id[token])\n",
    "            else:\n",
    "                sent_ids.append(word_to_id[unknown_token])\n",
    "        sample_report_ids.append(sent_ids)\n",
    "\n",
    "    sample_report_ids = np.array(sample_report_ids)\n",
    "    sample_report_ids = sample_report_ids.reshape(1, sample_report_ids.shape[0], sample_report_ids.shape[1])\n",
    "    #print(sample_report_ids.shape)\n",
    "#     one_hot_sample_report = dpt.one_hot_sequence(sample_report_ids, report_vocab_length)\n",
    "#     one_hot_sample_report = one_hot_sample_report[np.newaxis,:,:,:]\n",
    "    #print(one_hot_sample_report.shape)\n",
    "    prediction = predict_sequence(old_experiment,\n",
    "                                  sample_report_ids, \n",
    "                                  max_mesh_length, \n",
    "                                  id_to_mesh,\n",
    "                                  start_token=start_token,\n",
    "                                  end_token=end_token)\n",
    "    predicted_mesh_ids = one_hot_decode(prediction)\n",
    "    predicted_mesh = [id_to_mesh[idx] for idx in predicted_mesh_ids]\n",
    "    \n",
    "    sample_report = strip_start_end(sample_report)\n",
    "    predicted_mesh = strip_start_end(predicted_mesh)\n",
    "    \n",
    "    print('')\n",
    "    print('Original report: ', sample_report)\n",
    "    print('True mesh caption: ', true_mesh_caption)\n",
    "    print('Predicted mesh caption: ', predicted_mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate BLEU scores on all trian/val/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def evaluate_model(model, df, report_vocab_length):\n",
    "    actual, predicted = list(), list()\n",
    "    bleu1, bleu2, bleu3, bleu4 = list(), list(), list(), list()\n",
    "\n",
    "    for _, sample in df.iterrows():\n",
    "        true_mesh_caption = sample.single_mesh\n",
    "        sample_report = sample.pad_text_report\n",
    "\n",
    "        sample_report_ids = []\n",
    "        for sent in report:\n",
    "            sent_ids = []\n",
    "            for token in sent:\n",
    "                if token in word_to_id.keys():\n",
    "                    sent_ids.append(word_to_id[token])\n",
    "                else:\n",
    "                    sent_ids.append(word_to_id[unknown_token])\n",
    "            sample_report_ids.append(sent_ids)\n",
    "\n",
    "        sample_report_ids = np.array(sample_report_ids)\n",
    "        sample_report_ids = sample_report_ids.reshape(1, sample_report_ids.shape[0], sample_report_ids.shape[1])\n",
    "        #one_hot_sample_report = dpt.one_hot_sequence(sample_report_ids, report_vocab_length)\n",
    "\n",
    "        #target = predict_sequence(infenc, infdec, one_hot_sample_report, n_steps_out, n_features_out)\n",
    "        prediction = old_experiment.predict_sequence(sample_report_ids)\n",
    "        predicted_mesh_ids = one_hot_decode(prediction)\n",
    "        predicted_mesh = [id_to_mesh[idx] for idx in predicted_mesh_ids]\n",
    "\n",
    "        # sample_report = strip_start_end(sample_report)\n",
    "        yhat = strip_start_end(predicted_mesh)\n",
    "        reference = true_mesh_caption\n",
    "        \n",
    "        # calculate BLEU score\n",
    "        bleu1.append(sentence_bleu([reference], yhat, weights=(1.0, 0, 0, 0)))\n",
    "        bleu2.append(sentence_bleu([reference], yhat, weights=(0.5, 0.5, 0, 0)))\n",
    "        bleu3.append(sentence_bleu([reference], yhat, weights=(0.3, 0.3, 0.3, 0)))\n",
    "        bleu4.append(sentence_bleu([reference], yhat, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "    \n",
    "        # store actual and predicted\n",
    "        actual.append(reference)\n",
    "        predicted.append(yhat)\n",
    "        \n",
    "    print('BLEU1: ', np.mean(bleu1)*100)\n",
    "    print('BLEU2: ', np.mean(bleu2)*100)\n",
    "    print('BLEU3: ', np.mean(bleu3)*100)\n",
    "    print('BLEU4: ', np.mean(bleu4)*100)\n",
    "    \n",
    "    return actual, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_actual, train_predicted = evaluate_model(old_experiment, train_df, report_vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_actual, val_predicted = evaluate_model(old_experiment, val_df, report_vocab_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate ROUGE scores on all train/val/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rouge\n",
    "\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                       max_n=4,\n",
    "                       limit_length=True,\n",
    "                       length_limit=100,\n",
    "                       length_limit_type='words',\n",
    "                       apply_avg='Avg',\n",
    "                       apply_best='Best',\n",
    "                       alpha=0.5, # Default F1_score\n",
    "                       weight_factor=1.2,\n",
    "                       stemming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_results(p, r, f):\n",
    "    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hypotheses = [' '.join(p) for p in train_predicted]\n",
    "train_references = [' '.join(a) for a in train_actual]\n",
    "\n",
    "scores = evaluator.get_scores(train_hypotheses, train_references)\n",
    "\n",
    "for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "    print(prepare_results(results['p'], results['r'], results['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_hypotheses = [' '.join(p) for p in val_predicted]\n",
    "val_references = [' '.join(a) for a in val_actual]\n",
    "\n",
    "scores = evaluator.get_scores(val_hypotheses, val_references)\n",
    "\n",
    "for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "    print(prepare_results(results['p'], results['r'], results['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
