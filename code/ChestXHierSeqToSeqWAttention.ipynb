{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1210 14:06:33.595479 140602779563776 __init__.py:321] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "W1210 14:06:33.808887 140602779563776 __init__.py:321] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "W1210 14:06:33.959377 140602779563776 __init__.py:352] Limited tf.summary API due to missing TensorBoard installation.\n",
      "W1210 14:06:34.411010 140602779563776 __init__.py:321] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import json\n",
    "import sys\n",
    "# sys.path.append(\"..\")\n",
    "from utils import data_proc_tools as dpt\n",
    "from utils import plot_tools as pt\n",
    "from utils.custom_metrics import recall, precision, binary_accuracy\n",
    "from utils.custom_metrics import recall_np, precision_np, binary_accuracy_np, multilabel_confusion_matrix\n",
    "from utils.rnn_textsum_models import HierSeq2SeqAtt\n",
    "import random\n",
    "random.seed(42)\n",
    "random_state=1000\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import pylab\n",
    "\n",
    "pylab.rcParams['figure.figsize'] = (8.0, 10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "dir = '/vol/medic02/users/ag6516/radiology_report_summarisation/'\n",
    "data_dir = dir + 'data/'\n",
    "\n",
    "aug = 'aug'\n",
    "\n",
    "model_output_dir = dir + 'trained_models/hierseq2seq/'\n",
    "\n",
    "train_df = pd.read_pickle(data_dir + 'train/{}_train.pkl'.format(aug))\n",
    "val_df = pd.read_pickle(data_dir + 'val/val.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare sequence data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>examid</th>\n",
       "      <th>report</th>\n",
       "      <th>all_mesh</th>\n",
       "      <th>single_mesh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CXR1000_IM-0003</td>\n",
       "      <td>[increased, opacity, within, right, upper, lobe, possible, mass, associated, area, atelectasis, focal, consolidation, ., cardiac, silhouette, within, normal, limits, ., opacity, left, midlung, overlying, posterior, left, 5th, rib, may, represent, focal, airspace, disease, ., increased, opacity, right, upper, lobe, associated, atelectasis, may, represent, focal, consolidation, mass, lesion, atelectasis, ., recommend, chest, ct, evaluation, ., opacity, overlying, left, 5th, rib, may, represent, focal, airspace, disease]</td>\n",
       "      <td>[opacity, lung, lingula, opacity, lung, upper_lobe, right, pulmonary_atelectasis, upper_lobe, right]</td>\n",
       "      <td>[opacity, lung, upper_lobe, right]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CXR1001_IM-0004</td>\n",
       "      <td>[interstitial, markings, diffusely, prominent, throughout, lungs, ., heart, size, normal, ., pulmonary, normal, ., diffuse, fibrosis]</td>\n",
       "      <td>[diffuse, markings, lung, bilateral, interstitial, diffuse, prominent]</td>\n",
       "      <td>[markings, lung, bilateral, interstitial, diffuse, prominent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CXR1002_IM-0004</td>\n",
       "      <td>[status, post, left, mastectomy, ., heart, size, normal, ., lungs, clear]</td>\n",
       "      <td>[left]</td>\n",
       "      <td>[left]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CXR1003_IM-0005</td>\n",
       "      <td>[heart, size, pulmonary, vascularity, appear, within, normal, limits, ., retrocardiac, soft, tissue, density, present, ., appears, air, within, suggest, represents, hiatal, hernia, ., vascular, calcification, noted, ., calcified, granuloma, seen, ., interval, development, bandlike, opacity, left, lung, base, ., may, represent, atelectasis, ., osteopenia, present, spine, ., retrocardiac, soft, tissue, density, ., appearance, suggests, hiatal, hernia, ., left, base, bandlike, opacity, ., appearance, suggests, atelectasis]</td>\n",
       "      <td>[bone_diseases_metabolic, spine, calcified_granuloma, calcinosis, blood_vessels, density, retrocardiac, opacity, lung, base, left]</td>\n",
       "      <td>[opacity, lung, base, left]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CXR1004_IM-0005</td>\n",
       "      <td>[heart, ,, pulmonary, mediastinum, within, normal, limits, ., aorta, tortuous, ectatic, ., degenerative, changes, acromioclavicular, joints, ., degenerative, changes, spine, ., ivc, identified]</td>\n",
       "      <td>[aorta, tortuous, catheters_indwelling, shoulder, bilateral, degenerative, spine, degenerative]</td>\n",
       "      <td>[shoulder, bilateral, degenerative]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            examid  \\\n",
       "0  CXR1000_IM-0003   \n",
       "1  CXR1001_IM-0004   \n",
       "2  CXR1002_IM-0004   \n",
       "3  CXR1003_IM-0005   \n",
       "4  CXR1004_IM-0005   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          report  \\\n",
       "0  [increased, opacity, within, right, upper, lobe, possible, mass, associated, area, atelectasis, focal, consolidation, ., cardiac, silhouette, within, normal, limits, ., opacity, left, midlung, overlying, posterior, left, 5th, rib, may, represent, focal, airspace, disease, ., increased, opacity, right, upper, lobe, associated, atelectasis, may, represent, focal, consolidation, mass, lesion, atelectasis, ., recommend, chest, ct, evaluation, ., opacity, overlying, left, 5th, rib, may, represent, focal, airspace, disease]     \n",
       "1  [interstitial, markings, diffusely, prominent, throughout, lungs, ., heart, size, normal, ., pulmonary, normal, ., diffuse, fibrosis]                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "2  [status, post, left, mastectomy, ., heart, size, normal, ., lungs, clear]                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "3  [heart, size, pulmonary, vascularity, appear, within, normal, limits, ., retrocardiac, soft, tissue, density, present, ., appears, air, within, suggest, represents, hiatal, hernia, ., vascular, calcification, noted, ., calcified, granuloma, seen, ., interval, development, bandlike, opacity, left, lung, base, ., may, represent, atelectasis, ., osteopenia, present, spine, ., retrocardiac, soft, tissue, density, ., appearance, suggests, hiatal, hernia, ., left, base, bandlike, opacity, ., appearance, suggests, atelectasis]   \n",
       "4  [heart, ,, pulmonary, mediastinum, within, normal, limits, ., aorta, tortuous, ectatic, ., degenerative, changes, acromioclavicular, joints, ., degenerative, changes, spine, ., ivc, identified]                                                                                                                                                                                                                                                                                                                                               \n",
       "\n",
       "                                                                                                                             all_mesh  \\\n",
       "0  [opacity, lung, lingula, opacity, lung, upper_lobe, right, pulmonary_atelectasis, upper_lobe, right]                                 \n",
       "1  [diffuse, markings, lung, bilateral, interstitial, diffuse, prominent]                                                               \n",
       "2  [left]                                                                                                                               \n",
       "3  [bone_diseases_metabolic, spine, calcified_granuloma, calcinosis, blood_vessels, density, retrocardiac, opacity, lung, base, left]   \n",
       "4  [aorta, tortuous, catheters_indwelling, shoulder, bilateral, degenerative, spine, degenerative]                                      \n",
       "\n",
       "                                                     single_mesh  \n",
       "0  [opacity, lung, upper_lobe, right]                             \n",
       "1  [markings, lung, bilateral, interstitial, diffuse, prominent]  \n",
       "2  [left]                                                         \n",
       "3  [opacity, lung, base, left]                                    \n",
       "4  [shoulder, bilateral, degenerative]                            "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepend and append start and end tokens to mesh captions and text reports\n",
    "start_token = 'start'\n",
    "end_token = 'end'\n",
    "unknown_token = '**unknown**'\n",
    "max_mesh_length = 13 # avg. + 1std. + start + end\n",
    "max_num_words = 11 # avg. + 1std. + start + end, max number of words per sentence\n",
    "max_num_sentences = 6 # avg. + 1std. + start + end, max number of sentences per report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pad_report(report, max_num_sentences, max_num_words, start_token, end_token):\n",
    "    pad_seq = [end_token for x in range(max_num_words)]\n",
    "    r = ' '.join(report)\n",
    "    sentences = r.split(' . ')\n",
    "    padded_sentences = []\n",
    "    for sen in sentences:\n",
    "        words = sen.split(' ')\n",
    "        padded_sentence = dpt.pad_sequence(words, max_num_words, start_token, end_token)\n",
    "        padded_sentences.append(padded_sentence)\n",
    "    if len(padded_sentences) >= max_num_sentences:\n",
    "        padded_sentences = padded_sentences[:max_num_sentences]\n",
    "    else:\n",
    "        while len(padded_sentences) < max_num_sentences:\n",
    "            padded_sentences.append(pad_seq)\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['pad_mesh_caption'] = train_df.all_mesh.apply(lambda x: dpt.pad_sequence(x, max_mesh_length, start_token, end_token))\n",
    "train_df['pad_text_report'] = train_df.report.apply(lambda x: split_pad_report(x, max_num_sentences, max_num_words, start_token, end_token))\n",
    "\n",
    "val_df['pad_mesh_caption'] = val_df.all_mesh.apply(lambda x: dpt.pad_sequence(x, max_mesh_length, start_token, end_token))\n",
    "val_df['pad_text_report'] = val_df.report.apply(lambda x: split_pad_report(x, max_num_sentences, max_num_words, start_token, end_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorise text reports and mesh captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mesh = list(train_df.pad_mesh_caption)\n",
    "train_reports = list(train_df.pad_text_report)\n",
    "\n",
    "# vectorize train mesh captions\n",
    "dpt.mesh_to_vectors(train_mesh, dicts_dir=data_dir+'dicts/', \n",
    "                    load_dicts=True, save=True, \n",
    "                    output_dir=data_dir+'train/')\n",
    "\n",
    "# vectorise train reports\n",
    "vec_reports = []\n",
    "for report in train_reports:\n",
    "    vec = dpt.reports_to_vectors(report, \n",
    "                                 dicts_dir=data_dir+'dicts/', \n",
    "                                 load_dicts=True, \n",
    "                                 output_dir=data_dir+'train/')\n",
    "    vec_reports.append(vec)\n",
    "vec_reports = np.array(vec_reports)\n",
    "np.save(data_dir+'train/' + 'sent_token_ids_array.npy', vec_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_reports = list(val_df.pad_text_report)\n",
    "val_mesh = list(val_df.pad_mesh_caption)\n",
    "\n",
    "# vectorise val mesh using the same dict as created for train\n",
    "dpt.mesh_to_vectors(val_mesh, dicts_dir=data_dir+'dicts/', \n",
    "                    load_dicts=True, save=True, \n",
    "                    output_dir=data_dir+'val/')\n",
    "\n",
    "# vectorise val reports\n",
    "vec_reports = []\n",
    "for report in val_reports:\n",
    "    vec = dpt.reports_to_vectors(report, \n",
    "                                 dicts_dir=data_dir+'dicts/', \n",
    "                                 load_dicts=True, \n",
    "                                 output_dir=data_dir+'train/')\n",
    "    vec_reports.append(vec)\n",
    "vec_reports = np.array(vec_reports)\n",
    "np.save(data_dir+'val/' + 'sent_token_ids_array.npy', vec_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id, id_to_word = dpt.load_report_dicts(data_dir+'dicts/')\n",
    "mesh_to_id, id_to_mesh = dpt.load_mesh_dicts(data_dir+'dicts/')\n",
    "\n",
    "report_vocab_length = len(word_to_id)\n",
    "mesh_vocab_length = len(mesh_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1475, 128)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_vocab_length, mesh_vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays of indixes for input sentences, output entities and shifted output entities (t-1)\n",
    "train_token_ids_array = np.load(data_dir + 'train/sent_token_ids_array.npy')\n",
    "train_mesh_ids_array = np.load(data_dir + 'train/mesh_ids_array.npy')\n",
    "train_mesh_ids_array_shifted =[np.concatenate((mesh_to_id[start_token], t[:-1]), axis=None) for t in train_mesh_ids_array]\n",
    "train_mesh_ids_array_shifted = np.asarray(train_mesh_ids_array_shifted)\n",
    "\n",
    "val_token_ids_array = np.load(data_dir + 'val/sent_token_ids_array.npy')\n",
    "val_mesh_ids_array = np.load(data_dir + 'val/mesh_ids_array.npy')\n",
    "val_mesh_ids_array_shifted = [np.concatenate((mesh_to_id[start_token], t[:-1]), axis=None) for t in val_mesh_ids_array]\n",
    "val_mesh_ids_array_shifted = np.asarray(val_mesh_ids_array_shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot-encode\n",
    "# one_hot_reports_train = []\n",
    "# for report in train_token_ids_array:\n",
    "#     one_hot_reports_train.append(dpt.one_hot_sequence(report, report_vocab_length))\n",
    "# one_hot_reports_train = np.array(one_hot_reports_train)\n",
    "#dpt.one_hot_sequence(train_token_ids_array, report_vocab_length)\n",
    "one_hot_mesh_train = dpt.one_hot_sequence(train_mesh_ids_array, mesh_vocab_length)\n",
    "one_hot_mesh_shifted_train = dpt.one_hot_sequence(train_mesh_ids_array_shifted, mesh_vocab_length)\n",
    "\n",
    "# one_hot_reports_val = []\n",
    "# for report in val_token_ids_array:\n",
    "#     one_hot_reports_val.append(dpt.one_hot_sequence(report, report_vocab_length))\n",
    "# one_hot_reports_train = np.array(one_hot_reports_train)\n",
    "one_hot_mesh_val = dpt.one_hot_sequence(val_mesh_ids_array, mesh_vocab_length)\n",
    "one_hot_mesh_shifted_val = dpt.one_hot_sequence(val_mesh_ids_array_shifted, mesh_vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5148, 13, 128), (5148, 13, 128))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_mesh_train.shape, one_hot_mesh_shifted_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mesh_ids_array[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Seq-to-Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word encoder emb:  (None, 11, 128)\n",
      "Sent emb:  (None, 64)\n",
      "Encoder outputs:  (None, 6, 64)\n",
      "Encoder state:  (None, 64)\n",
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_18 (InputLayer)           [(None, 6, 11)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_6 (TimeDistrib (None, 6, 64)        238208      input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           [(None, None, 128)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   [(None, 6, 64), (Non 33024       time_distributed_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   multiple             49408       input_19[0][0]                   \n",
      "                                                                 lstm_7[0][1]                     \n",
      "                                                                 lstm_7[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "attention_layer (AttentionLayer multiple             8256        lstm_7[0][0]                     \n",
      "                                                                 lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "concat_layer (Concatenate)      (None, None, 128)    0           lstm_8[0][0]                     \n",
      "                                                                 attention_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_7 (TimeDistrib (None, None, 128)    16512       concat_layer[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 345,408\n",
      "Trainable params: 345,408\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from utils.custom_metrics import recall, precision, binary_accuracy\n",
    "from utils.rnn_textsum_models import HierSeq2SeqAtt\n",
    "\n",
    "input_dim = len(word_to_id)\n",
    "output_dim = len(mesh_to_id)\n",
    "hidden_dim = 64\n",
    "encoder_emb_dim = 128\n",
    "input_word_seq_length = max_num_words\n",
    "input_sent_seq_length = max_num_sentences\n",
    "output_seq_length = max_mesh_length\n",
    "epochs = 50\n",
    "optimizer = 'adam'\n",
    "loss='categorical_crossentropy'\n",
    "batch_size = 128\n",
    "\n",
    "new_experiment = HierSeq2SeqAtt(epochs=epochs,\n",
    "                               metrics=['accuracy', binary_accuracy,recall,precision],\n",
    "                               optimizer=optimizer,\n",
    "                               loss=loss,\n",
    "                               batch_size=batch_size, \n",
    "                               input_dim=input_dim,\n",
    "                               output_dim=output_dim,\n",
    "                               hidden_dim=hidden_dim,\n",
    "                               encoder_emb_dim=encoder_emb_dim,\n",
    "                               input_word_seq_length=input_word_seq_length,\n",
    "                               input_sent_seq_length=input_sent_seq_length,\n",
    "                               output_seq_length=output_seq_length,\n",
    "                               verbose=True)\n",
    "new_experiment.build_model()\n",
    "new_experiment.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batch generators\n",
    "# train_batch_generator = dpt.batch_generator_seq2seq(train_token_ids_array, report_vocab_length, train_mesh_ids_array, \n",
    "#                                                    train_mesh_ids_array_shifted, mesh_vocab_length, batch_size)\n",
    "\n",
    "# val_batch_generator = dpt.batch_generator_seq2seq(val_token_ids_array, report_vocab_length, val_mesh_ids_array, \n",
    "#                                                    val_mesh_ids_array_shifted, mesh_vocab_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word encoder emb:  (None, 11, 128)\n",
      "Sent emb:  (None, 64)\n",
      "Encoder outputs:  (None, 6, 64)\n",
      "Encoder state:  (None, 64)\n",
      "Train on 5148 samples, validate on 300 samples\n",
      "Epoch 1/50\n",
      "5148/5148 [==============================] - 9s 2ms/sample - loss: 2.9574 - accuracy: 0.5281 - binary_accuracy: 0.9935 - recall: 0.2852 - precision: 0.4583 - val_loss: 1.9513 - val_accuracy: 0.6526 - val_binary_accuracy: 0.9950 - val_recall: 0.5052 - val_precision: 0.7870\n",
      "Epoch 2/50\n",
      "5148/5148 [==============================] - 2s 437us/sample - loss: 2.0800 - accuracy: 0.5971 - binary_accuracy: 0.9951 - recall: 0.4937 - precision: 0.8144 - val_loss: 1.6771 - val_accuracy: 0.6574 - val_binary_accuracy: 0.9964 - val_recall: 0.5946 - val_precision: 0.9209\n",
      "Epoch 3/50\n",
      "5148/5148 [==============================] - 2s 438us/sample - loss: 1.8393 - accuracy: 0.6158 - binary_accuracy: 0.9961 - recall: 0.5378 - precision: 0.9372 - val_loss: 1.4854 - val_accuracy: 0.7031 - val_binary_accuracy: 0.9968 - val_recall: 0.6098 - val_precision: 0.9769\n",
      "Epoch 4/50\n",
      "5148/5148 [==============================] - 2s 435us/sample - loss: 1.6951 - accuracy: 0.6436 - binary_accuracy: 0.9964 - recall: 0.5591 - precision: 0.9578 - val_loss: 1.4091 - val_accuracy: 0.7031 - val_binary_accuracy: 0.9968 - val_recall: 0.6024 - val_precision: 0.9868\n",
      "Epoch 5/50\n",
      "5148/5148 [==============================] - 2s 430us/sample - loss: 1.6114 - accuracy: 0.6489 - binary_accuracy: 0.9965 - recall: 0.5815 - precision: 0.9592 - val_loss: 1.3334 - val_accuracy: 0.7097 - val_binary_accuracy: 0.9971 - val_recall: 0.6485 - val_precision: 0.9737\n",
      "Epoch 6/50\n",
      "5148/5148 [==============================] - 2s 441us/sample - loss: 1.5461 - accuracy: 0.6555 - binary_accuracy: 0.9966 - recall: 0.5852 - precision: 0.9623 - val_loss: 1.2836 - val_accuracy: 0.7126 - val_binary_accuracy: 0.9971 - val_recall: 0.6548 - val_precision: 0.9679\n",
      "Epoch 7/50\n",
      "5148/5148 [==============================] - 2s 438us/sample - loss: 1.4814 - accuracy: 0.6655 - binary_accuracy: 0.9966 - recall: 0.5906 - precision: 0.9641 - val_loss: 1.2276 - val_accuracy: 0.7241 - val_binary_accuracy: 0.9971 - val_recall: 0.6532 - val_precision: 0.9747\n",
      "Epoch 8/50\n",
      "5148/5148 [==============================] - 2s 435us/sample - loss: 1.4115 - accuracy: 0.6833 - binary_accuracy: 0.9967 - recall: 0.5940 - precision: 0.9663 - val_loss: 1.1694 - val_accuracy: 0.7390 - val_binary_accuracy: 0.9972 - val_recall: 0.6666 - val_precision: 0.9706\n",
      "Epoch 9/50\n",
      "5148/5148 [==============================] - 2s 438us/sample - loss: 1.3309 - accuracy: 0.6996 - binary_accuracy: 0.9968 - recall: 0.6055 - precision: 0.9687 - val_loss: 1.1138 - val_accuracy: 0.7474 - val_binary_accuracy: 0.9973 - val_recall: 0.6736 - val_precision: 0.9696\n",
      "Epoch 10/50\n",
      "5148/5148 [==============================] - 2s 434us/sample - loss: 1.2592 - accuracy: 0.7124 - binary_accuracy: 0.9968 - recall: 0.6119 - precision: 0.9693 - val_loss: 1.0552 - val_accuracy: 0.7600 - val_binary_accuracy: 0.9973 - val_recall: 0.6864 - val_precision: 0.9656\n",
      "Epoch 11/50\n",
      "5148/5148 [==============================] - 2s 440us/sample - loss: 1.1883 - accuracy: 0.7242 - binary_accuracy: 0.9969 - recall: 0.6191 - precision: 0.9710 - val_loss: 0.9969 - val_accuracy: 0.7715 - val_binary_accuracy: 0.9974 - val_recall: 0.6862 - val_precision: 0.9740\n",
      "Epoch 12/50\n",
      "5148/5148 [==============================] - 2s 439us/sample - loss: 1.1195 - accuracy: 0.7380 - binary_accuracy: 0.9970 - recall: 0.6313 - precision: 0.9724 - val_loss: 0.9533 - val_accuracy: 0.7769 - val_binary_accuracy: 0.9974 - val_recall: 0.7003 - val_precision: 0.9641\n",
      "Epoch 13/50\n",
      "5148/5148 [==============================] - 2s 432us/sample - loss: 1.0589 - accuracy: 0.7501 - binary_accuracy: 0.9970 - recall: 0.6397 - precision: 0.9679 - val_loss: 0.9128 - val_accuracy: 0.7846 - val_binary_accuracy: 0.9975 - val_recall: 0.7004 - val_precision: 0.9751\n",
      "Epoch 14/50\n",
      "5148/5148 [==============================] - 2s 436us/sample - loss: 1.0049 - accuracy: 0.7614 - binary_accuracy: 0.9971 - recall: 0.6540 - precision: 0.9684 - val_loss: 0.8778 - val_accuracy: 0.7944 - val_binary_accuracy: 0.9975 - val_recall: 0.7103 - val_precision: 0.9645\n",
      "Epoch 15/50\n",
      "5148/5148 [==============================] - 2s 439us/sample - loss: 0.9578 - accuracy: 0.7714 - binary_accuracy: 0.9972 - recall: 0.6626 - precision: 0.9650 - val_loss: 0.8491 - val_accuracy: 0.7979 - val_binary_accuracy: 0.9976 - val_recall: 0.7120 - val_precision: 0.9722\n",
      "Epoch 16/50\n",
      "5148/5148 [==============================] - 2s 437us/sample - loss: 0.9161 - accuracy: 0.7783 - binary_accuracy: 0.9973 - recall: 0.6754 - precision: 0.9663 - val_loss: 0.8193 - val_accuracy: 0.8033 - val_binary_accuracy: 0.9976 - val_recall: 0.7267 - val_precision: 0.9664\n",
      "Epoch 17/50\n",
      "5148/5148 [==============================] - 2s 440us/sample - loss: 0.8735 - accuracy: 0.7876 - binary_accuracy: 0.9973 - recall: 0.6853 - precision: 0.9649 - val_loss: 0.8014 - val_accuracy: 0.8097 - val_binary_accuracy: 0.9976 - val_recall: 0.7320 - val_precision: 0.9622\n",
      "Epoch 18/50\n",
      "5148/5148 [==============================] - 2s 429us/sample - loss: 0.8386 - accuracy: 0.7950 - binary_accuracy: 0.9974 - recall: 0.6949 - precision: 0.9658 - val_loss: 0.7783 - val_accuracy: 0.8118 - val_binary_accuracy: 0.9977 - val_recall: 0.7405 - val_precision: 0.9645\n",
      "Epoch 19/50\n",
      "5148/5148 [==============================] - 2s 471us/sample - loss: 0.8064 - accuracy: 0.8012 - binary_accuracy: 0.9975 - recall: 0.7047 - precision: 0.9647 - val_loss: 0.7691 - val_accuracy: 0.8105 - val_binary_accuracy: 0.9978 - val_recall: 0.7435 - val_precision: 0.9679\n",
      "Epoch 20/50\n",
      "5148/5148 [==============================] - 2s 439us/sample - loss: 0.7759 - accuracy: 0.8078 - binary_accuracy: 0.9976 - recall: 0.7131 - precision: 0.9654 - val_loss: 0.7468 - val_accuracy: 0.8210 - val_binary_accuracy: 0.9978 - val_recall: 0.7522 - val_precision: 0.9621\n",
      "Epoch 21/50\n",
      "5148/5148 [==============================] - 2s 456us/sample - loss: 0.7452 - accuracy: 0.8146 - binary_accuracy: 0.9976 - recall: 0.7207 - precision: 0.9648 - val_loss: 0.7326 - val_accuracy: 0.8241 - val_binary_accuracy: 0.9978 - val_recall: 0.7587 - val_precision: 0.9643\n",
      "Epoch 22/50\n",
      "5148/5148 [==============================] - 3s 507us/sample - loss: 0.7209 - accuracy: 0.8210 - binary_accuracy: 0.9977 - recall: 0.7265 - precision: 0.9629 - val_loss: 0.7258 - val_accuracy: 0.8218 - val_binary_accuracy: 0.9978 - val_recall: 0.7611 - val_precision: 0.9592\n",
      "Epoch 23/50\n",
      "5148/5148 [==============================] - 2s 477us/sample - loss: 0.6972 - accuracy: 0.8257 - binary_accuracy: 0.9977 - recall: 0.7344 - precision: 0.9636 - val_loss: 0.7162 - val_accuracy: 0.8251 - val_binary_accuracy: 0.9979 - val_recall: 0.7665 - val_precision: 0.9568\n",
      "Epoch 24/50\n",
      "5148/5148 [==============================] - 2s 485us/sample - loss: 0.6726 - accuracy: 0.8319 - binary_accuracy: 0.9978 - recall: 0.7413 - precision: 0.9630 - val_loss: 0.7152 - val_accuracy: 0.8274 - val_binary_accuracy: 0.9979 - val_recall: 0.7736 - val_precision: 0.9506\n",
      "Epoch 25/50\n",
      "5148/5148 [==============================] - 2s 480us/sample - loss: 0.6546 - accuracy: 0.8365 - binary_accuracy: 0.9978 - recall: 0.7473 - precision: 0.9626 - val_loss: 0.7059 - val_accuracy: 0.8331 - val_binary_accuracy: 0.9979 - val_recall: 0.7778 - val_precision: 0.9509\n",
      "Epoch 26/50\n",
      "5148/5148 [==============================] - 4s 847us/sample - loss: 0.6297 - accuracy: 0.8434 - binary_accuracy: 0.9979 - recall: 0.7539 - precision: 0.9639 - val_loss: 0.6874 - val_accuracy: 0.8331 - val_binary_accuracy: 0.9979 - val_recall: 0.7797 - val_precision: 0.9538\n",
      "Epoch 27/50\n",
      "5148/5148 [==============================] - 5s 932us/sample - loss: 0.6113 - accuracy: 0.8464 - binary_accuracy: 0.9979 - recall: 0.7615 - precision: 0.9631 - val_loss: 0.6849 - val_accuracy: 0.8328 - val_binary_accuracy: 0.9979 - val_recall: 0.7792 - val_precision: 0.9538\n",
      "Epoch 28/50\n",
      "5148/5148 [==============================] - 5s 956us/sample - loss: 0.5906 - accuracy: 0.8524 - binary_accuracy: 0.9980 - recall: 0.7691 - precision: 0.9645 - val_loss: 0.6851 - val_accuracy: 0.8362 - val_binary_accuracy: 0.9980 - val_recall: 0.7813 - val_precision: 0.9612\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5148/5148 [==============================] - 5s 934us/sample - loss: 0.5779 - accuracy: 0.8552 - binary_accuracy: 0.9980 - recall: 0.7719 - precision: 0.9631 - val_loss: 0.6780 - val_accuracy: 0.8372 - val_binary_accuracy: 0.9979 - val_recall: 0.7900 - val_precision: 0.9451\n",
      "Epoch 30/50\n",
      "5148/5148 [==============================] - 5s 929us/sample - loss: 0.5543 - accuracy: 0.8620 - binary_accuracy: 0.9981 - recall: 0.7809 - precision: 0.9652 - val_loss: 0.6653 - val_accuracy: 0.8413 - val_binary_accuracy: 0.9980 - val_recall: 0.7904 - val_precision: 0.9505\n",
      "Epoch 31/50\n",
      "5148/5148 [==============================] - 5s 936us/sample - loss: 0.5357 - accuracy: 0.8657 - binary_accuracy: 0.9981 - recall: 0.7864 - precision: 0.9653 - val_loss: 0.6673 - val_accuracy: 0.8397 - val_binary_accuracy: 0.9980 - val_recall: 0.7924 - val_precision: 0.9456\n",
      "Epoch 32/50\n",
      "5148/5148 [==============================] - 5s 918us/sample - loss: 0.5242 - accuracy: 0.8682 - binary_accuracy: 0.9981 - recall: 0.7915 - precision: 0.9653 - val_loss: 0.6581 - val_accuracy: 0.8413 - val_binary_accuracy: 0.9980 - val_recall: 0.7991 - val_precision: 0.9456\n",
      "Epoch 33/50\n",
      "5148/5148 [==============================] - 5s 960us/sample - loss: 0.5073 - accuracy: 0.8725 - binary_accuracy: 0.9982 - recall: 0.7953 - precision: 0.9659 - val_loss: 0.6612 - val_accuracy: 0.8431 - val_binary_accuracy: 0.9980 - val_recall: 0.7979 - val_precision: 0.9500\n",
      "Epoch 34/50\n",
      "5148/5148 [==============================] - 5s 941us/sample - loss: 0.4941 - accuracy: 0.8766 - binary_accuracy: 0.9982 - recall: 0.8010 - precision: 0.9669 - val_loss: 0.6526 - val_accuracy: 0.8464 - val_binary_accuracy: 0.9980 - val_recall: 0.8055 - val_precision: 0.9425\n",
      "Epoch 35/50\n",
      "5148/5148 [==============================] - 5s 916us/sample - loss: 0.4769 - accuracy: 0.8810 - binary_accuracy: 0.9983 - recall: 0.8082 - precision: 0.9669 - val_loss: 0.6607 - val_accuracy: 0.8462 - val_binary_accuracy: 0.9980 - val_recall: 0.8031 - val_precision: 0.9423\n",
      "Epoch 36/50\n",
      "5148/5148 [==============================] - 5s 930us/sample - loss: 0.4639 - accuracy: 0.8838 - binary_accuracy: 0.9983 - recall: 0.8122 - precision: 0.9682 - val_loss: 0.6608 - val_accuracy: 0.8428 - val_binary_accuracy: 0.9980 - val_recall: 0.8026 - val_precision: 0.9428\n",
      "Epoch 37/50\n",
      "5148/5148 [==============================] - 5s 933us/sample - loss: 0.4537 - accuracy: 0.8859 - binary_accuracy: 0.9984 - recall: 0.8165 - precision: 0.9679 - val_loss: 0.6633 - val_accuracy: 0.8436 - val_binary_accuracy: 0.9980 - val_recall: 0.8070 - val_precision: 0.9372\n",
      "Epoch 38/50\n",
      "5148/5148 [==============================] - 5s 948us/sample - loss: 0.4386 - accuracy: 0.8901 - binary_accuracy: 0.9984 - recall: 0.8217 - precision: 0.9684 - val_loss: 0.6597 - val_accuracy: 0.8456 - val_binary_accuracy: 0.9980 - val_recall: 0.8101 - val_precision: 0.9365\n",
      "Epoch 39/50\n",
      "5148/5148 [==============================] - 4s 709us/sample - loss: 0.4267 - accuracy: 0.8926 - binary_accuracy: 0.9984 - recall: 0.8265 - precision: 0.9687 - val_loss: 0.6613 - val_accuracy: 0.8444 - val_binary_accuracy: 0.9980 - val_recall: 0.8073 - val_precision: 0.9395\n",
      "Epoch 40/50\n",
      "5148/5148 [==============================] - 5s 928us/sample - loss: 0.4166 - accuracy: 0.8956 - binary_accuracy: 0.9985 - recall: 0.8301 - precision: 0.9683 - val_loss: 0.6650 - val_accuracy: 0.8444 - val_binary_accuracy: 0.9980 - val_recall: 0.8096 - val_precision: 0.9364\n",
      "Epoch 41/50\n",
      "5148/5148 [==============================] - 5s 930us/sample - loss: 0.4044 - accuracy: 0.8986 - binary_accuracy: 0.9985 - recall: 0.8347 - precision: 0.9701 - val_loss: 0.6675 - val_accuracy: 0.8451 - val_binary_accuracy: 0.9980 - val_recall: 0.8109 - val_precision: 0.9291\n",
      "Epoch 42/50\n",
      "5148/5148 [==============================] - 5s 939us/sample - loss: 0.3911 - accuracy: 0.9019 - binary_accuracy: 0.9985 - recall: 0.8392 - precision: 0.9713 - val_loss: 0.6615 - val_accuracy: 0.8459 - val_binary_accuracy: 0.9980 - val_recall: 0.8107 - val_precision: 0.9321\n",
      "Epoch 43/50\n",
      "5148/5148 [==============================] - 5s 950us/sample - loss: 0.3802 - accuracy: 0.9052 - binary_accuracy: 0.9986 - recall: 0.8425 - precision: 0.9714 - val_loss: 0.6626 - val_accuracy: 0.8464 - val_binary_accuracy: 0.9980 - val_recall: 0.8092 - val_precision: 0.9304\n",
      "Epoch 44/50\n",
      "5148/5148 [==============================] - 5s 937us/sample - loss: 0.3706 - accuracy: 0.9080 - binary_accuracy: 0.9986 - recall: 0.8473 - precision: 0.9723 - val_loss: 0.6673 - val_accuracy: 0.8456 - val_binary_accuracy: 0.9980 - val_recall: 0.8146 - val_precision: 0.9293\n",
      "Epoch 45/50\n",
      "5148/5148 [==============================] - 5s 965us/sample - loss: 0.3586 - accuracy: 0.9104 - binary_accuracy: 0.9986 - recall: 0.8507 - precision: 0.9717 - val_loss: 0.6691 - val_accuracy: 0.8472 - val_binary_accuracy: 0.9980 - val_recall: 0.8126 - val_precision: 0.9300\n",
      "Epoch 46/50\n",
      "5148/5148 [==============================] - 5s 960us/sample - loss: 0.3487 - accuracy: 0.9139 - binary_accuracy: 0.9987 - recall: 0.8559 - precision: 0.9729 - val_loss: 0.6765 - val_accuracy: 0.8510 - val_binary_accuracy: 0.9980 - val_recall: 0.8163 - val_precision: 0.9265\n",
      "Epoch 47/50\n",
      "5148/5148 [==============================] - 5s 928us/sample - loss: 0.3416 - accuracy: 0.9149 - binary_accuracy: 0.9987 - recall: 0.8569 - precision: 0.9726 - val_loss: 0.6736 - val_accuracy: 0.8485 - val_binary_accuracy: 0.9980 - val_recall: 0.8169 - val_precision: 0.9283\n",
      "Epoch 48/50\n",
      "5148/5148 [==============================] - 5s 916us/sample - loss: 0.3363 - accuracy: 0.9162 - binary_accuracy: 0.9987 - recall: 0.8587 - precision: 0.9733 - val_loss: 0.6710 - val_accuracy: 0.8454 - val_binary_accuracy: 0.9980 - val_recall: 0.8129 - val_precision: 0.9296\n",
      "Epoch 49/50\n",
      "5148/5148 [==============================] - 5s 933us/sample - loss: 0.3314 - accuracy: 0.9178 - binary_accuracy: 0.9987 - recall: 0.8620 - precision: 0.9725 - val_loss: 0.6615 - val_accuracy: 0.8482 - val_binary_accuracy: 0.9981 - val_recall: 0.8186 - val_precision: 0.9286\n",
      "Epoch 50/50\n",
      "5148/5148 [==============================] - 5s 943us/sample - loss: 0.3191 - accuracy: 0.9211 - binary_accuracy: 0.9988 - recall: 0.8665 - precision: 0.9739 - val_loss: 0.6706 - val_accuracy: 0.8479 - val_binary_accuracy: 0.9981 - val_recall: 0.8200 - val_precision: 0.9285\n"
     ]
    }
   ],
   "source": [
    "new_experiment.run_experiment(train_token_ids_array, one_hot_mesh_shifted_train, one_hot_mesh_train, \n",
    "                              val_token_ids_array, one_hot_mesh_shifted_val, one_hot_mesh_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_name = 'hierseq2seq_att'\n",
    "model_output_dir = dir + 'trained_models/{}/'.format(model_name)\n",
    "new_experiment.save_weights_history(model_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results of specific experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'hierseq2seq_att'\n",
    "model_output_dir = dir + 'trained_models/{}/'.format(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word encoder emb:  (None, 11, 128)\n",
      "Sent emb:  (None, 64)\n",
      "Encoder outputs:  (None, 6, 64)\n",
      "Encoder state:  (None, 64)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "model_name = 'hierseq2seq'\n",
    "epochs = 50\n",
    "encoder_emb_dim = 128\n",
    "decoder_emb_dim = 256\n",
    "hidden_dim = 64\n",
    "\n",
    "param_fn = 'param_encoderembdim_{}_decoderembdim_{}_hiddendim_{}_epochs_{}.pkl'.format(encoder_emb_dim,\n",
    "                                                                               decoder_emb_dim,\n",
    "                                                                               hidden_dim,\n",
    "                                                                               epochs)\n",
    "params = pickle.load(open(model_output_dir + param_fn, 'rb'))\n",
    "\n",
    "old_experiment = HierSeq2SeqAtt(**params)\n",
    "old_experiment.build_model()\n",
    "old_experiment.load_weights_history(model_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decode a one hot encoded string\n",
    "def one_hot_decode(encoded_seq):\n",
    "    return [np.argmax(vector) for vector in encoded_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_start_end(seq, start_token='start', end_token='end'):\n",
    "    stripped_seq = []\n",
    "    for s in seq:\n",
    "        if s not in [start_token, end_token]:\n",
    "            stripped_seq.append(s)\n",
    "    return stripped_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(experiment, source, max_seq_len, id_to_mesh, start_token='start', end_token='end'):\n",
    "    # encode source\n",
    "    enc_outs, h, c = experiment.encoder_model.predict(source)\n",
    "    enc_state = [h,c]\n",
    "    dec_state = enc_state\n",
    "\n",
    "    # start of sequence input\n",
    "    in_text = [start_token]\n",
    "\n",
    "    # integer encoder\n",
    "    in_seq_ids = dpt.mesh_to_vectors([in_text], dicts_dir=data_dir+'dicts/', \n",
    "                   load_dicts=True, save=False)\n",
    "    # one-hot encode\n",
    "    in_seq_onehot = dpt.one_hot_sequence(in_seq_ids, mesh_vocab_length)\n",
    "    in_seq_onehot = np.array(in_seq_onehot)\n",
    "    in_seq_onehot = in_seq_onehot.reshape(1, 1, in_seq_onehot.shape[-1])\n",
    "    target_seq = in_seq_onehot\n",
    "    \n",
    "    # collect predictions\n",
    "    output = []\n",
    "    attention_weights = []\n",
    "    max_att = []\n",
    "    max_att2 = []\n",
    "    for t in range(max_seq_len):\n",
    "        dec_out, attention, h, c  = experiment.decoder_model.predict([enc_outs] + dec_state + [target_seq])\n",
    "        dec_state = [h,c]\n",
    "        # store prediction\n",
    "        #output.append(dec_out[0,0,:])\n",
    "        dec_ind = np.argmax(dec_out, axis=-1)[0, 0]\n",
    "        output.append(dec_ind)\n",
    "        #print(dec_ind)\n",
    "        attention_weights.append((dec_ind, attention))\n",
    "        idx = np.argmax(attention, axis=-1)[0, 0]\n",
    "        max_att.append(np.argmax(attention, axis=-1)[0, 0])\n",
    "        attention[:,:,idx] = 0\n",
    "        max_att2.append(np.argmax(attention, axis=-1)[0, 0])\n",
    "        target_seq = dec_out\n",
    "\n",
    "    #predicted_mesh_ids = one_hot_decode(output)\n",
    "    predicted_mesh = [id_to_mesh[i] for i in output]\n",
    "    \n",
    "    return predicted_mesh, attention_weights, max_att, max_att2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original report:  [['start', 'interval', 'cabg', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'sternotomy', 'appear', 'intact', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'stable', ',', 'mild', 'degenerative', 'disc', 'disease', 'thoracic', 'spine', 'end', 'end'], ['start', 'visualized', 'bony', 'structures', 'otherwise', 'unremarkable', 'appearance', 'end', 'end', 'end', 'end'], ['start', 'atherosclerotic', 'calcifications', 'thoracic', 'aorta', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'clear', 'lungs', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end']]\n",
      "True mesh caption:  ['atherosclerosis', 'aorta_thoracic', 'thoracic_vertebrae', 'degenerative', 'mild']\n",
      "Predicted mesh caption:  ['aorta_thoracic', 'tortuous', 'mild', 'thoracic_vertebrae', 'thoracic_vertebrae', 'degenerative', 'mild']\n",
      "Attention word inputs 1:  [['start', 'sternotomy', 'appear', 'intact', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'atherosclerotic', 'calcifications', 'thoracic', 'aorta', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'atherosclerotic', 'calcifications', 'thoracic', 'aorta', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'atherosclerotic', 'calcifications', 'thoracic', 'aorta', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'atherosclerotic', 'calcifications', 'thoracic', 'aorta', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'stable', ',', 'mild', 'degenerative', 'disc', 'disease', 'thoracic', 'spine', 'end', 'end'], ['start', 'stable', ',', 'mild', 'degenerative', 'disc', 'disease', 'thoracic', 'spine', 'end', 'end'], ['start', 'stable', ',', 'mild', 'degenerative', 'disc', 'disease', 'thoracic', 'spine', 'end', 'end'], ['start', 'clear', 'lungs', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'clear', 'lungs', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'clear', 'lungs', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'clear', 'lungs', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'clear', 'lungs', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end']]\n",
      "Attention word inputs 2:  [['start', 'clear', 'lungs', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'stable', ',', 'mild', 'degenerative', 'disc', 'disease', 'thoracic', 'spine', 'end', 'end'], ['start', 'stable', ',', 'mild', 'degenerative', 'disc', 'disease', 'thoracic', 'spine', 'end', 'end'], ['start', 'stable', ',', 'mild', 'degenerative', 'disc', 'disease', 'thoracic', 'spine', 'end', 'end'], ['start', 'stable', ',', 'mild', 'degenerative', 'disc', 'disease', 'thoracic', 'spine', 'end', 'end'], ['start', 'atherosclerotic', 'calcifications', 'thoracic', 'aorta', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'atherosclerotic', 'calcifications', 'thoracic', 'aorta', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'clear', 'lungs', 'end', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'sternotomy', 'appear', 'intact', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'sternotomy', 'appear', 'intact', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'sternotomy', 'appear', 'intact', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'sternotomy', 'appear', 'intact', 'end', 'end', 'end', 'end', 'end', 'end', 'end'], ['start', 'visualized', 'bony', 'structures', 'otherwise', 'unremarkable', 'appearance', 'end', 'end', 'end', 'end']]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(1):\n",
    "    sample = val_df.sample(1, random_state=42)\n",
    "    true_mesh_caption = list(sample.all_mesh)[0]\n",
    "    sample_report = list(sample.pad_text_report)[0]\n",
    "    \n",
    "    sample_report_ids = []\n",
    "    for sent in sample_report:\n",
    "        sent_ids = []\n",
    "        for token in sent:\n",
    "            if token in word_to_id.keys():\n",
    "                sent_ids.append(word_to_id[token])\n",
    "            else:\n",
    "                sent_ids.append(word_to_id[unknown_token])\n",
    "        sample_report_ids.append(sent_ids)\n",
    "\n",
    "    sample_report_ids = np.array(sample_report_ids)\n",
    "    sample_report_ids = sample_report_ids.reshape(1, sample_report_ids.shape[0], sample_report_ids.shape[1])\n",
    "    #print(sample_report_ids.shape)\n",
    "#     one_hot_sample_report = dpt.one_hot_sequence(sample_report_ids, report_vocab_length)\n",
    "#     one_hot_sample_report = one_hot_sample_report[np.newaxis,:,:,:]\n",
    "    #print(one_hot_sample_report.shape)\n",
    "    prediction, attention_weights, max_att1, max_att2 = predict_sequence(old_experiment,\n",
    "                                  sample_report_ids, \n",
    "                                  max_mesh_length, \n",
    "                                  id_to_mesh,\n",
    "                                  start_token=start_token,\n",
    "                                  end_token=end_token)\n",
    "    #predicted_mesh_ids = one_hot_decode(prediction)\n",
    "    #predicted_mesh = [id_to_mesh[idx] for idx in predicted_mesh_ids]\n",
    "    \n",
    "    sample_report = strip_start_end(sample_report)\n",
    "    predicted_mesh = strip_start_end(prediction)\n",
    "    \n",
    "    print('')\n",
    "    print('Original report: ', sample_report)\n",
    "    print('True mesh caption: ', true_mesh_caption)\n",
    "    print('Predicted mesh caption: ', predicted_mesh)\n",
    "    \n",
    "    att_words1 = [sample_report[k] for k in max_att1]\n",
    "    att_words2 = [sample_report[k] for k in max_att2]\n",
    "\n",
    "    print('Attention word inputs 1: ', att_words1)\n",
    "    print('Attention word inputs 2: ', att_words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate BLEU scores on all trian/val/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def evaluate_model(model, df, report_vocab_length):\n",
    "    actual, predicted = list(), list()\n",
    "    bleu1, bleu2, bleu3, bleu4 = list(), list(), list(), list()\n",
    "\n",
    "    for _, sample in df.iterrows():\n",
    "        true_mesh_caption = sample.all_mesh\n",
    "        sample_report = sample.pad_text_report\n",
    "        \n",
    "        sample_report_ids = []\n",
    "        for sent in sample_report:\n",
    "            sent_ids = []\n",
    "            for token in sent:\n",
    "                if token in word_to_id.keys():\n",
    "                    sent_ids.append(word_to_id[token])\n",
    "                else:\n",
    "                    sent_ids.append(word_to_id[unknown_token])\n",
    "            sample_report_ids.append(sent_ids)\n",
    "\n",
    "        sample_report_ids = np.array(sample_report_ids)\n",
    "        sample_report_ids = sample_report_ids.reshape(1, sample_report_ids.shape[0], sample_report_ids.shape[1])\n",
    "        prediction, _, _, _= predict_sequence(old_experiment,\n",
    "                                      sample_report_ids, \n",
    "                                      max_mesh_length, \n",
    "                                      id_to_mesh,\n",
    "                                      start_token=start_token,\n",
    "                                      end_token=end_token)\n",
    "\n",
    "        yhat = strip_start_end(prediction)\n",
    "        reference = true_mesh_caption\n",
    "        \n",
    "        # calculate BLEU score\n",
    "        bleu1.append(sentence_bleu([reference], yhat, weights=(1.0, 0, 0, 0)))\n",
    "        bleu2.append(sentence_bleu([reference], yhat, weights=(0.5, 0.5, 0, 0)))\n",
    "        bleu3.append(sentence_bleu([reference], yhat, weights=(0.3, 0.3, 0.3, 0)))\n",
    "        bleu4.append(sentence_bleu([reference], yhat, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "    \n",
    "        # store actual and predicted\n",
    "        actual.append(reference)\n",
    "        predicted.append(yhat)\n",
    "        \n",
    "    print('BLEU1: ', np.mean(bleu1)*100)\n",
    "    print('BLEU2: ', np.mean(bleu2)*100)\n",
    "    print('BLEU3: ', np.mean(bleu3)*100)\n",
    "    print('BLEU4: ', np.mean(bleu4)*100)\n",
    "    \n",
    "    return actual, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU1:  75.61706725973593\n",
      "BLEU2:  45.61712572897954\n",
      "BLEU3:  32.5762069658353\n",
      "BLEU4:  19.894382214974662\n"
     ]
    }
   ],
   "source": [
    "train_actual, train_predicted = evaluate_model(old_experiment, train_df.sample(2000), report_vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/medic02/users/ag6516/python3env/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/vol/medic02/users/ag6516/python3env/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/vol/medic02/users/ag6516/python3env/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU1:  67.27627032252504\n",
      "BLEU2:  25.2024659697054\n",
      "BLEU3:  15.010061038851074\n",
      "BLEU4:  6.140149916104892\n"
     ]
    }
   ],
   "source": [
    "val_actual, val_predicted = evaluate_model(old_experiment, val_df, report_vocab_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate ROUGE scores on all train/val/test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rouge\n",
    "\n",
    "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                       max_n=4,\n",
    "                       limit_length=True,\n",
    "                       length_limit=100,\n",
    "                       length_limit_type='words',\n",
    "                       apply_avg='Avg',\n",
    "                       apply_best='Best',\n",
    "                       alpha=0.5, # Default F1_score\n",
    "                       weight_factor=1.2,\n",
    "                       stemming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_results(p, r, f):\n",
    "    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hypotheses = [' '.join(p) for p in train_predicted]\n",
    "train_references = [' '.join(a) for a in train_actual]\n",
    "\n",
    "scores = evaluator.get_scores(train_hypotheses, train_references)\n",
    "\n",
    "for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "    print(prepare_results(results['p'], results['r'], results['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\trouge-1:\tP: 76.09\tR: 71.92\tF1: 72.54\n",
      "\trouge-2:\tP: 31.44\tR: 28.17\tF1: 28.46\n",
      "\trouge-3:\tP: 19.87\tR: 18.53\tF1: 18.45\n",
      "\trouge-4:\tP: 10.20\tR:  9.70\tF1:  9.54\n",
      "\trouge-l:\tP: 77.17\tR: 73.51\tF1: 74.24\n",
      "\trouge-w:\tP: 72.60\tR: 58.96\tF1: 62.89\n"
     ]
    }
   ],
   "source": [
    "val_hypotheses = [' '.join(p) for p in val_predicted]\n",
    "val_references = [' '.join(a) for a in val_actual]\n",
    "\n",
    "scores = evaluator.get_scores(val_hypotheses, val_references)\n",
    "\n",
    "for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "    print(prepare_results(results['p'], results['r'], results['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
